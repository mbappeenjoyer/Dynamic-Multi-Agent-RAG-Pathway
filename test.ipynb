{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tGnDDJYDPtdM"
   },
   "source": [
    "### Installing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Ho08BInaDZBC"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install pathway litellm\n",
    "!mkdir -p sample_documents\n",
    "!pip install PyMuPDF\n",
    "!pip install llama-index\n",
    "!pip install groq\n",
    "!pip install llama-index-llms-groq\n",
    "!pip install llama-index-embeddings-together\n",
    "!pip install -U pathway\n",
    "!pip install pathway[llm-docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XUpecSFSRlcx"
   },
   "outputs": [],
   "source": [
    "!pip install PyPDF2\n",
    "!pip install -qU llama-index\n",
    "!pip install llama-index-core==0.10.6.post1\n",
    "!pip install llama-index-embeddings-huggingface\n",
    "!pip install llama-index-embeddings-instructor\n",
    "!pip install llama-index-embeddings-together\n",
    "!pip install llama-index-llms-together\n",
    "!pip install together\n",
    "!pip install llama-index-tools-tavily-research\n",
    "!pip install -qU langchain\n",
    "!pip install -qU langchain_experimental\n",
    "!pip install -qU langchain-together\n",
    "!pip install -qU langchain_huggingface\n",
    "!pip install -qU faiss-gpu\n",
    "!mkdir -p data\n",
    "!pip install -U accelerate\n",
    "!pip install llama-index-packs-raptor\n",
    "!pip install langchain-groq\n",
    "!pip install llama-index-llms-groq\n",
    "!pip install unstructured-client\n",
    "!pip install pymupdf pillow\n",
    "!pip install faiss-cpu\n",
    "!pip install pdfplumber\n",
    "!pip install -U sentence-transformers\n",
    "!pip install einops\n",
    "!pip install llama-index-embeddings-langchain\n",
    "!pip install -q nmslib\n",
    "!pip install llama-index-embeddings-jinaai\n",
    "!pip install -q guardrails-ai\n",
    "!pip install -U nemoguardrails\n",
    "%pip install --upgrade langchain-together\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LRDiEOT0Rlcy"
   },
   "source": [
    "### API Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "id": "YHbHgns6P5nb"
   },
   "outputs": [],
   "source": [
    "unstructured_api  = \"TQCVUxpxUYIQkjCiSqA1j3skEyM8rB\"\n",
    "supervisor_groq_api = \"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\"\n",
    "rag_agent_api = \"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\"\n",
    "raptor_api = \"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\"\n",
    "JINAAI_API_KEY = \"jina_f1254b13a77b43338c55e3e95891f1a7oqLU3r9laWMOzYOLkpQcXW1zSf9O\"\n",
    "summary_groq_api = \"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\"\n",
    "embed_jinna_api = \"jina_f1254b13a77b43338c55e3e95891f1a7oqLU3r9laWMOzYOLkpQcXW1zSf9O\"\n",
    "os.environ['OPENAI_API_KEY'] = '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'\n",
    "os.environ['OPENAI_API_BASE'] = 'https://api.groq.com/openai/v1/chat/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "id": "b7DjHul6Csa_"
   },
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ndfrU9xsJBhj"
   },
   "source": [
    "### Imports and Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "id": "_qw4stAVTmi8"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import faiss\n",
    "import os\n",
    "from copy import deepcopy\n",
    "import inspect\n",
    "import sys\n",
    "from io import StringIO\n",
    "import sys\n",
    "import time\n",
    "import re\n",
    "import traceback\n",
    "import logging\n",
    "import subprocess\n",
    "from llama_index.core.memory import VectorMemory\n",
    "from llama_index.core.llms import ChatMessage\n",
    "from typing import List, Union\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from llama_index.core.tools import FunctionTool\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import time\n",
    "from groq import Groq\n",
    "import fitz\n",
    "from llama_index.core import Document\n",
    "from llama_index.packs.raptor.base import RaptorRetriever\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "from llama_index.llms.groq import Groq as llama_groq\n",
    "from llama_index.core import Settings\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.embeddings.together import TogetherEmbedding\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from PyPDF2 import PdfReader, PdfWriter\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "import random\n",
    "import unstructured_client\n",
    "from unstructured_client.models import operations, shared\n",
    "from llama_index.embeddings.jinaai import JinaEmbedding\n",
    "import gc\n",
    "import sys\n",
    "import re\n",
    "from llama_index.core.tools import FunctionTool\n",
    "import numpy as np\n",
    "import time\n",
    "from llama_index.tools.tavily_research import TavilyToolSpec\n",
    "import nltk\n",
    "import torch\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "from typing import List\n",
    "from collections import namedtuple\n",
    "import nmslib\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xWB3c0m08nR7"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "# import wikipedia\n",
    "from bs4 import BeautifulSoup\n",
    "from langchain_community.document_loaders import WikipediaLoader\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "import subprocess\n",
    "import re\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "import chromadb\n",
    "from llama_index.core.llama_pack import download_llama_pack\n",
    "RaptorPack = download_llama_pack(\"RaptorPack\", \"./raptor_pack\")\n",
    "from llama_index.packs.raptor import RaptorRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GWzaG3z48RJx"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import nltk\n",
    "download_dir = '/usr/local/share/nltk_data'\n",
    "nltk.download('punkt_tab', download_dir=download_dir)\n",
    "nltk.data.path.append(download_dir)\n",
    "nltk.download('averaged_perceptron_tagger_eng', download_dir=download_dir)\n",
    "nltk.data.path.append(download_dir)\n",
    "nltk.download('averaged_perceptron_tagger', download_dir=download_dir)\n",
    "nltk.data.path.append(download_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D5G2IAHaSTpT",
    "outputId": "04ebbe3f-c7df-4f83-c131-c5a6f6cc15b5"
   },
   "outputs": [],
   "source": [
    "import pexpect\n",
    "\n",
    "def automate_guardrails_configure(api_key):\n",
    "    # Increase the timeout for commands\n",
    "    process = pexpect.spawn(\"guardrails configure\", timeout=30)\n",
    "\n",
    "    try:\n",
    "        # Respond to the first prompt (anonymous metrics)\n",
    "        process.expect(\"Enable anonymous metrics reporting\\\\? \\\\[Y/n\\\\]:\")\n",
    "        process.sendline(\"y\")\n",
    "\n",
    "        # Respond to the second prompt (remote inferencing)\n",
    "        process.expect(\"Do you wish to use remote inferencing\\\\? \\\\[Y/n\\\\]:\")\n",
    "        process.sendline(\"y\")\n",
    "\n",
    "        # Provide the API key\n",
    "        process.expect(\"Enter API Key below.*:\")\n",
    "        process.sendline(api_key)\n",
    "\n",
    "        # Allow the process to complete\n",
    "        process.expect(pexpect.EOF)\n",
    "        print(\"Configuration completed successfully.\")\n",
    "\n",
    "    except pexpect.TIMEOUT:\n",
    "        print(\"The process timed out. Check if the command is taking too long or if the prompts are matching correctly.\")\n",
    "        print(process.before.decode())\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "        print(process.before.decode())\n",
    "\n",
    "# Replace 'your_actual_api_key_here' with the actual API key\n",
    "api_key = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiJnb29nbGUtb2F1dGgyfDEwNzMzNjQ1MjQ5OTA5NDA2MzQyMiIsImFwaUtleUlkIjoiNmU3ODk2OTMtNzk2Yy00Njc5LTkwOWMtMzc5MmU3MWJlNGNhIiwic2NvcGUiOiJyZWFkOnBhY2thZ2VzIiwiaWF0IjoxNzMyOTg3MTM1LCJleHAiOjE3NDA3NjMxMzV9.mvME5I7cS6mShraX036-gmWboUbxmJ-naridvB0uNlM\"\n",
    "automate_guardrails_configure(api_key)\n",
    "\n",
    "import asyncio\n",
    "try:\n",
    "    asyncio.get_event_loop()\n",
    "except RuntimeError:\n",
    "    asyncio.set_event_loop(asyncio.new_event_loop())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "Rd91ZEORRlcz"
   },
   "outputs": [],
   "source": [
    "class ModuleInstallError(Exception):\n",
    "    \"\"\"Custom exception for module installation issues.\"\"\"\n",
    "    pass\n",
    "class ToolError(Exception):\n",
    "    pass\n",
    "class APIError(Exception):\n",
    "  \"\"\"\n",
    "  Exception to represent API errors.\n",
    "  \"\"\"\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "fS0dwAjMRlcz"
   },
   "outputs": [],
   "source": [
    "# Supervisor\n",
    "chat_llm= ChatGroq(model=\"llama-3.1-70b-versatile\", api_key = supervisor_groq_api, temperature=0.1)\n",
    "#RAG AGENT\n",
    "chat_llm1 = ChatGroq(model=\"llama-3.1-70b-versatile\", api_key = rag_agent_api)\n",
    "#Raptor\n",
    "llm = llama_groq(model=\"llama3-70b-8192\", api_key = raptor_api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "F1EajszJhPTE"
   },
   "outputs": [],
   "source": [
    "pdf_loader = unstructured_client.UnstructuredClient( api_key_auth = unstructured_api, server_url=\"https://api.unstructuredapp.io\", )\n",
    "Settings.llm = llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "pQwQxIubUNQt"
   },
   "outputs": [],
   "source": [
    "text_embed_model = JinaEmbedding(\n",
    "    api_key=embed_jinna_api,\n",
    "    model=\"jina-embeddings-v3\",\n",
    "    task=\"retrieval.passage\",\n",
    ")\n",
    "query_embed_model = JinaEmbedding(\n",
    "    api_key=embed_jinna_api,\n",
    "    model=\"jina-embeddings-v3\",\n",
    "    task=\"retrieval.query\",\n",
    "    dimensions=1024,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "8F-vrBYKVwOO"
   },
   "outputs": [],
   "source": [
    "# we maintain a global output stream , since we execute exec() on strings that have print statements within , so to access\n",
    "# we switch to a temporary output string. during this process error may occur and we may losse the entire output stream\n",
    "out = sys.stdout  #Important"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4RAxkMI-SA9c"
   },
   "source": [
    "### Guardrails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z44_8uxMSF-v"
   },
   "outputs": [],
   "source": [
    "from guardrails.hub import ToxicLanguage\n",
    "from guardrails import Guard\n",
    "import re\n",
    "\n",
    "# Set up the Guardrails guard with the ToxicLanguage validation\n",
    "guard = Guard().use(\n",
    "    ToxicLanguage, threshold=0.5,\n",
    "    validation_method=\"sentence\",\n",
    "    on_fail=\"exception\"\n",
    ")\n",
    "\n",
    "# Function to validate and return non-toxic segments\n",
    "def filter_toxic_segments(text):\n",
    "    # Split by both periods and commas, keeping each segment\n",
    "    segments = re.split(r\"[.]\", text)\n",
    "    non_toxic_segments = []\n",
    "\n",
    "    for segment in segments:\n",
    "        if segment.strip():\n",
    "            try:\n",
    "                guard.validate(segment.strip())\n",
    "                non_toxic_segments.append(segment.strip())\n",
    "            except Exception as e:\n",
    "                # If toxic, skip this segment\n",
    "                pass\n",
    "\n",
    "\n",
    "    return \". \".join(non_toxic_segments)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ojZoefjlSFx5"
   },
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from nemoguardrails import LLMRails, RailsConfig\n",
    "from nemoguardrails.integrations.langchain.runnable_rails import RunnableRails\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "config = RailsConfig.from_path(\"/content/config (1).yml\")\n",
    "\n",
    "guardrails = RunnableRails(config)\n",
    "output_parser = StrOutputParser()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BdjHCGbBvZvB"
   },
   "outputs": [],
   "source": [
    "import asyncio\n",
    "try:\n",
    "    asyncio.get_event_loop()\n",
    "except RuntimeError:\n",
    "    asyncio.set_event_loop(asyncio.new_event_loop())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UuwBxjQQnnU_"
   },
   "source": [
    "### Interleaving RAG Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ieul7ZVXb0yT"
   },
   "source": [
    "#### Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "-va_spKOqlM3"
   },
   "outputs": [],
   "source": [
    "confidence_prompt = \"\"\"You will be provided with a series of interleaved reasoning and retrieval steps which have led to a FINAL ANSWER, that will also be provided to you.\n",
    "Based on the series of steps, you are to generate a CONFIDENCE SCORE for the FINAL ANSWER generated. The score must lie between 0 and 1 where a higher score means a greater confidence\n",
    "in the generated answer.\n",
    "\n",
    "- If final answer does not fully answer the query , then give low score\n",
    "\n",
    "RETURN ONLY THE SCORE.\n",
    "\n",
    "Now,\n",
    "HISTORY OF SERIES OF INTERLEAVED REASONING AND RETRIEVAL : {steps},\n",
    "FINAL ANSWER : {answer}\n",
    "\n",
    "CONFIDENCE SCORE:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "TDIAl7_ihSRY"
   },
   "outputs": [],
   "source": [
    "THOUGHT_PROMPT = \"\"\"\n",
    "As a Thought Agent, your role is to systematically analyze questions through retrieval and reasoning steps.\n",
    "You will be provided with an AGENT INPUT containing previous Thoughts, Observations, and Reasoning steps.\n",
    "Using this information, you need to determine whether the next step should be a RETRIEVAL or REASONING, and what the current\n",
    "Thought is for which retrieval or reasoning would be performed. You should observe the previous steps and the given question\n",
    "to determine the next direction.\n",
    "\n",
    "\n",
    "IMPORTANT : THERE IS NO NEED TO GENERATE FURTHER RETRIEVAL OR REASONING CALLS ONCE YOU REALISE THE NECCESSARY CONTEXT TO ANSWER THE QUESTION HAS BEEN GATHERED.\n",
    "ALWAYS CHECK THE AVAILABLE CONTEXT (given as agent input below) BEFORE CHOOSING.\n",
    "\n",
    "\n",
    "Here's how to approach this process:\n",
    "\n",
    "```\n",
    "EXAMPLE :\n",
    "User Query : \"How many NVIDIA P100 GPUs were used to train the transformer model?\"\n",
    "\n",
    "RETRIEVAL THOUGHT : How many NVIDIA P100 GPUs were used to train the transformer model?\n",
    "RAG ACTION: How many NVIDIA P100 GPUs were used to train the transformer model? --> 8 #output of RAG Action\n",
    "REASONING THOUGHT : Based on the provided context, the number of GPUs was 8.\n",
    "FINAL ANSWER : The number of GPUs used to train the transformer model was 8.\n",
    "\n",
    "(End of Example)\n",
    "```\n",
    "\n",
    "Guidelines for thoughts:\n",
    "\n",
    "RETRIEVAL THOUGHTS should:\n",
    "- Directly ask for specific pieces of needed data in question format without mentioning unnecessary context about \"documents\" or \"Acts\" or \"reports\". For example, instead of \"What is the definition of total disablement under the Workmen's Compensation Act?\" simply ask, \"What is the definition of total disablement?\"\n",
    "- DO NOT ask for specific entities that would other wise be generated from raw content, eg. what is the gross margin trend. Always ask for the raw data such as the gross margins in this case, and you will reason on this later.\n",
    "- Be clear about what information is being sought.\n",
    "\n",
    "Example: \"What is Coca Cola's total dividends paid for FY2022?\"\n",
    "\n",
    "REASONING THOUGHTS should:\n",
    "- State what calculation or analysis is needed\n",
    "- Break complex calculations into steps\n",
    "- Show clear mathematical working\n",
    "\n",
    "Example: \"Calculate the ratio by dividing total dividends by net income\"\n",
    "\n",
    "Key principles:\n",
    "\n",
    "1. Each thought builds on previous ones\n",
    "2. Clear progression from data gathering to calculation\n",
    "3. Explicit about data sources and calculations\n",
    "4. Shows step-by-step working\n",
    "5. Provides a formatted final answer\n",
    "\n",
    "I hope you understood the example.\n",
    "\n",
    "AT EACH STEP YOU HAVE TO GENERATE A RETRIEVAL OR A REASONING THOUGHT.\n",
    "\n",
    "DO NOT POINTLESSLY RE-GENERATE ANY THOUGHT. MAKE USE OF THE CONTEXT AND RE-USE THOSE ANSWERS WHENEVER NEEDED.\n",
    "\n",
    "YOU CANNOT JUMP TO THE FINAL ANSWER.\n",
    "ALWAYS GATHER CONTEXT AND USE IT TO BUILD YOUR ANSWER UP. PLEASE.\n",
    "\n",
    "Now,\n",
    "\n",
    "QUESTION: {question}\n",
    "Below is the current conversation consisting of interleaving human and assistant messages.\n",
    "AGENT INPUT: {agent_input}\n",
    "\n",
    "OUTPUT:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "wHtMIlqqhSh0"
   },
   "outputs": [],
   "source": [
    "REASONING_PROMPT = \"\"\"\n",
    "\n",
    "Your role is to perform self reasoning for a given reasoning thought to help solve a provided question. You must follow the following steps:\n",
    "\n",
    "\n",
    " 1.You will be provided with an AGENT INPUT containing previous Thoughts, Observations, and Reasoning steps.\n",
    " 2.You have to follow the last generated REASONING THOUGHT in the AGENT INPUT and perform reasoning as instructed in the THOUGHT.\n",
    " 3.You can use the past observations and Reasonings to perform the reasoning.\n",
    " 4.Perform complete calculations/substitution and computaion.\n",
    "\n",
    "\n",
    "Make sure all the above instructions are followed.\n",
    "\n",
    "Please use the following output format:\n",
    "```\n",
    "REASONING: <your reasoning>\n",
    "```\n",
    "\n",
    "If the current reasoning step is the final answer to the input question and adresses it completely, use the following output format:\n",
    "\n",
    "```\n",
    "FINAL ANSWER: <your reasoning with all observation>\n",
    "```\n",
    "\n",
    "MAKE SURE FINAL ANSWER IS COMPLETE\n",
    "STRICTLY INLCUDE ALL THE REQUIRED INFORMATION IN FINAL ANSWER [AFTER `FINAL ASNWER:`]\n",
    "ONLY CONTENT AFTER `FINAL ANSWER :` WILL WE CONSIDERED AS FINAL ANSWER\n",
    "\n",
    "```\n",
    "Example Question: \"What is Coca Cola's FY2022 dividend payout ratio (using total cash dividends paid and net income attributable to shareholders)? Round answer to two decimal places.\"\n",
    "Example Flow:\n",
    "\n",
    "RETRIEVAL THOUGHT: What is Coca Cola's total cash dividends paid for FY2022?\n",
    "Observation: Located in the cash flow statement: Total cash dividends paid for FY2022 = $7,578 million\n",
    "RETRIEVAL THOUGHT: What is Coca Cola's net income attributable to shareholders for FY2022?\n",
    "Observation: Located in the income statement: Net income attributable to shareholders for FY2022 = $9,542 million\n",
    "REASONING THOUGHT: Calculate dividend payout ratio using the formula:\n",
    "Dividend Payout Ratio = Total Cash Dividends Paid / Net Income Attributable to Shareholders\n",
    "REASONING: Let's calculate step by step:\n",
    "1. Dividend Payout Ratio = $7,578 million / $9,542 million\n",
    "2. = 0.7940\n",
    "3. Round to two decimal places = 0.79\n",
    "\n",
    "FINAL ANSWER: As Total cash dividends paid is $7,578 million and Net income attributable to shareholder is $9,542 million so we can conclude that Coca Cola's FY2022 dividend payout ratio was 0.79 or 79%.\n",
    "\n",
    "(End of Example)\n",
    "```\n",
    "ONLY CONTENT AFTER `FINAL ANSWER :` WILL WE CONSIDERED AS FINAL ANSWER\n",
    "NOTHING IS SUPPOSED TO BE GENERATED AFTER THE FINAL ANSWER.\n",
    "\n",
    "Now,\n",
    "\n",
    "QUESTION: {question}\n",
    "AGENT INPUT: {agent_input}\n",
    "\n",
    "\n",
    "OUTPUT:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "wr10bYkiq4Sc"
   },
   "outputs": [],
   "source": [
    "JARGON_IDENTIFY_PROMPT = \"\"\"\n",
    "As an expert Jargon Identifier, you will analyze a user query to identify any technical jargon, abbreviations, or specialized terms. Ensure that you examine each word and phrase carefully, as some jargon may be subtle or field-specific.\n",
    "\n",
    "Follow the output format specified below: ['jargon1', 'jargon2', 'jargon3',...]. Properly wrap each identified jargon term in a list separated by commas.\n",
    "\n",
    "If there are no jargon terms present, RETURN JUST the string None.\n",
    "STRICTLY FOLLOW THE OUTPUT FORMAT MENTIONED ABOVE\n",
    "DO NOT RETURN ANY OTHER TEXT\n",
    "\n",
    "Now, User query: {query}\n",
    "Prev Jargons :  {prev_jargons}\n",
    "\n",
    "OUTPUT:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "R2JMeG4jq5Ym"
   },
   "outputs": [],
   "source": [
    "REPHRASE_PROMPT = \"\"\"\n",
    "Your task is to add clarifications to all instances of jargon in the provided query using the exact meanings or formulas from the given Jargon Dictionary. The goal is to ensure the query is understandable while maintaining its original context and intent.\n",
    "\n",
    "**Rules:\n",
    "\n",
    "- Add clarifications for jargon terms by including their corresponding definitions or formulas from the Jargon Dictionary alongside the original jargon term.\n",
    "- If the definition includes a formula, include it in parentheses after the term, preserving the jargon term itself.\n",
    "- If the jargon term is defined only with a formula, include both the original term and the formula together in the rephrased query.\n",
    "- Do not remove or replace the jargon terms; instead, enhance the query by embedding clarifications.\n",
    "- Ensure the revised query retains its readability, coherence, and natural flow without introducing unrelated changes.\n",
    "- Use the provided dictionary exclusively for clarifications; do not infer or add any meanings beyond those provided.\n",
    "- Ensure the rephrased query fully aligns with the original query in content and context.\n",
    "\n",
    "STRICTLY FOLLOW THE ABOVE RULES\n",
    "RETURN ONLY REPHRASED QUERY , NOT ANY CLARIFICATION AND TEXT\n",
    "\n",
    "**Input:**\n",
    "Query: {query}\n",
    "Jargon Dictionary: {jargons}\n",
    "\n",
    "Output:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "id": "FV1y6sLjq75A"
   },
   "outputs": [],
   "source": [
    "jargon_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a jargon detecting agent.\"),\n",
    "    (\"human\", JARGON_IDENTIFY_PROMPT)\n",
    "])\n",
    "\n",
    "rephrase_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a query rephraser agent.\"),\n",
    "    (\"human\",  REPHRASE_PROMPT)\n",
    "])\n",
    "\n",
    "thought_agent_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful Thought Generating Agent.\"),\n",
    "    (\"human\", THOUGHT_PROMPT),\n",
    "])\n",
    "\n",
    "reasoning_agent_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful Reasoning Agent.\"),\n",
    "    (\"human\", REASONING_PROMPT),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DDGsuiT9b30p"
   },
   "source": [
    "#### Memory Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "JSdtCdJJcBI9"
   },
   "outputs": [],
   "source": [
    "class DynamicCacheIndex:\n",
    "    def __init__(self,\n",
    "                 dim: int = 768,\n",
    "                 index_type: str = 'hnsw',\n",
    "                 space: str = 'cosinesimil',\n",
    "                 batch_size: int = 32):\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.metadata = {}\n",
    "        self.embeddings = []\n",
    "        self.id_counter = 0\n",
    "        self.index_created = False\n",
    "        self.pending_additions = [] #we perform additions to the HNSW graph in batches.\n",
    "        self.text_embed_model = None\n",
    "\n",
    "        # Initializing the HNSW index\n",
    "        self.index = nmslib.init(method=index_type, space=space)\n",
    "\n",
    "        # Initializing the embedding model\n",
    "        if not self.text_embed_model:\n",
    "          try:\n",
    "              self._init_embedding_model()\n",
    "          except Exception as e:\n",
    "              pass\n",
    "\n",
    "    def _init_embedding_model(self) -> None:\n",
    "        \"\"\"Initialize the embedding model with error handling\"\"\"\n",
    "        try:\n",
    "            jina_api_key = JINAAI_API_KEY\n",
    "            if not jina_api_key:\n",
    "                raise ValueError(\"JINAAI_API_KEY environment variable not set\")\n",
    "\n",
    "            self.text_embed_model = JinaEmbedding(\n",
    "                api_key=jina_api_key,\n",
    "                model=\"jina-embeddings-v3\",\n",
    "            )\n",
    "        except Exception as e:\n",
    "            raise\n",
    "\n",
    "    def process_pending_additions(self, force=False) -> bool:\n",
    "        \"\"\"Process pending additions with force option\"\"\"\n",
    "        if not self.pending_additions and not force:\n",
    "            return False\n",
    "\n",
    "        try:\n",
    "            batches = [self.pending_additions[i:i + self.batch_size]\n",
    "                      for i in range(0, len(self.pending_additions), self.batch_size)]\n",
    "\n",
    "            with tqdm(total=len(batches), desc=\"Processing batches\") as pbar:\n",
    "                for batch in batches:\n",
    "                    for embedding, metadata in batch:\n",
    "                        if not isinstance(embedding, np.ndarray):\n",
    "                            embedding = np.array(embedding)\n",
    "\n",
    "                        if embedding.shape[0] != self.dim:\n",
    "                            raise ValueError(f\"Embedding dimension mismatch. Expected {self.dim}, got {embedding.shape[0]}\")\n",
    "\n",
    "                        self.index.addDataPoint(self.id_counter, embedding)\n",
    "                        self.metadata[self.id_counter] = metadata\n",
    "                        self.embeddings.append(embedding)\n",
    "                        self.id_counter += 1\n",
    "                    pbar.update(1)\n",
    "\n",
    "            # Clear pending additions\n",
    "            self.pending_additions = []\n",
    "\n",
    "            # Recreate index with progress tracking\n",
    "            self.index.createIndex(\n",
    "                {'post': 2},\n",
    "                print_progress=True\n",
    "            )\n",
    "            self.index_created = True\n",
    "            return True\n",
    "\n",
    "        except Exception as e:\n",
    "            return False\n",
    "\n",
    "    def add_chunk(self, chunk: str, query_metadata: str = None) -> Optional[int]:\n",
    "        \"\"\"Add a chunk to the index with validation and metadata handling\"\"\"\n",
    "        if not chunk:\n",
    "            return None\n",
    "\n",
    "        try:\n",
    "            if not self.text_embed_model:\n",
    "                self._init_embedding_model()\n",
    "\n",
    "            chunk_str = str(chunk)\n",
    "\n",
    "            metadata = {}\n",
    "            if query_metadata:\n",
    "                try:\n",
    "                    # Try to parse the metadata if it's a JSON string\n",
    "                    if isinstance(query_metadata, str):\n",
    "                        metadata = json.loads(query_metadata)\n",
    "                    elif isinstance(query_metadata, dict):\n",
    "                        metadata = query_metadata\n",
    "                    else:\n",
    "                        metadata = {'original_metadata': query_metadata}\n",
    "                except (json.JSONDecodeError, TypeError):\n",
    "                    # If parsing fails, store as is\n",
    "                    metadata = {'original_metadata': query_metadata}\n",
    "\n",
    "            if 'chunk' not in metadata:\n",
    "                metadata['chunk'] = chunk_str\n",
    "\n",
    "            chunk_embedding = self.text_embed_model.get_text_embedding(chunk_str)\n",
    "\n",
    "            if not isinstance(chunk_embedding, (list, np.ndarray)):\n",
    "                raise ValueError(\"Invalid embedding format\")\n",
    "\n",
    "            if isinstance(chunk_embedding, list):\n",
    "                chunk_embedding = np.array(chunk_embedding)\n",
    "\n",
    "            # Validate embedding dimension\n",
    "            if chunk_embedding.shape[0] != self.dim:\n",
    "                raise ValueError(f\"Embedding dimension mismatch. Expected {self.dim}, got {chunk_embedding.shape[0]}\")\n",
    "\n",
    "            self.pending_additions.append((\n",
    "                chunk_embedding,\n",
    "                metadata\n",
    "            ))\n",
    "\n",
    "            # Process if batch size reached\n",
    "            if len(self.pending_additions) >= self.batch_size:\n",
    "                self.process_pending_additions()\n",
    "\n",
    "            return self.id_counter\n",
    "\n",
    "        except Exception as e:\n",
    "            return None\n",
    "\n",
    "    def search(self,\n",
    "              query_vector: np.ndarray,\n",
    "              k: int = 5) -> List[Tuple[int, float, Dict]]:\n",
    "        \"\"\"Search the index with metadata retrieval\"\"\"\n",
    "        if not isinstance(query_vector, np.ndarray):\n",
    "            try:\n",
    "                query_vector = np.array(query_vector)\n",
    "            except Exception as e:\n",
    "                return []\n",
    "\n",
    "        if query_vector.shape[0] != self.dim:\n",
    "            print(f\"Query vector dimension mismatch. Expected {self.dim}, got {query_vector.shape[0]}\")\n",
    "            return []\n",
    "\n",
    "        try:\n",
    "            # Process any pending additions\n",
    "            if self.pending_additions:\n",
    "                if not self.process_pending_additions():\n",
    "                    print(\"Failed to process pending additions\")\n",
    "                    return []\n",
    "\n",
    "            if not self.index_created or len(self.embeddings) == 0:\n",
    "                print(\"Index not created or empty\")\n",
    "                return []\n",
    "\n",
    "            k = min(k, len(self.embeddings))\n",
    "            ids, distances = self.index.knnQuery(query_vector, k=k) # retrieves the closest node and top k neighbours in the graph\n",
    "\n",
    "            results = []\n",
    "            for i, chunk_id in enumerate(ids):\n",
    "                metadata = self.metadata.get(int(chunk_id), {})\n",
    "\n",
    "                result_metadata = {\n",
    "                    'query': metadata.get('query', 'No query found'),\n",
    "                    'chunk': metadata.get('chunk', 'No chunk found'),\n",
    "                    'query_type': metadata.get('query_type', 'unknown'),\n",
    "                    'original_metadata': metadata\n",
    "                }\n",
    "\n",
    "                results.append((\n",
    "                    int(chunk_id),\n",
    "                    float(distances[i]),\n",
    "                    result_metadata\n",
    "                ))\n",
    "\n",
    "            return results\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error during search: {e}\")\n",
    "            return []\n",
    "\n",
    "    def save_index(self, filename: str, save_data: bool = True) -> None:\n",
    "        \"\"\"Save the index with error handling\"\"\"\n",
    "        try:\n",
    "            self.index.saveIndex(filename, save_data)\n",
    "\n",
    "            metadata_file = f\"{filename}_metadata.json\"\n",
    "            with open(metadata_file, 'w') as f:\n",
    "                json.dump({str(k): v for k, v in self.metadata.items()}, f)\n",
    "\n",
    "        except Exception as e:\n",
    "            raise\n",
    "\n",
    "    def load_index(self, filename: str) -> None:\n",
    "        \"\"\"Load the index with error handling\"\"\"\n",
    "        try:\n",
    "            self.index.loadIndex(filename)\n",
    "\n",
    "            metadata_file = f\"{filename}_metadata.json\"\n",
    "            with open(metadata_file, 'r') as f:\n",
    "                self.metadata = {int(k): v for k, v in json.load(f).items()}\n",
    "\n",
    "            self.index_created = True\n",
    "\n",
    "        except Exception as e:\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "NV_lWbtVcNdJ"
   },
   "outputs": [],
   "source": [
    "def create_utility_query_prompt(template=None, number=3, data=None):\n",
    "    \"\"\"\n",
    "    Create a dynamic utility query generation prompt\n",
    "\n",
    "    Args:\n",
    "    - template: Optional custom template\n",
    "    - number: Number of queries to generate\n",
    "    - data: The actual data chunk to generate queries for\n",
    "    \"\"\"\n",
    "    default_template = f\"\"\"You are an expert query generator agent. Given the data below, generate {number} distinct queries. Ensure each query is:\n",
    "1. Single-hop (focuses on one specific aspect)\n",
    "2. Clear and concise\n",
    "3. Unique and relevant\n",
    "4. All queries should have a clear and direct answer in the data, it shouldn't be ambiguous.\n",
    "\n",
    "Respond STRICTLY in this EXACT JSON format:\n",
    "{{\n",
    "    \"query_1\": \"First query text here\",\n",
    "    \"query_2\": \"First query text here\",\n",
    "    \"query_3\": \"First query text here\"\n",
    "}}\n",
    "\n",
    "DATA:\n",
    "{data}\n",
    "\n",
    "Your Response:\"\"\"\n",
    "\n",
    "    if template:\n",
    "        return template.format(data=data)\n",
    "    return default_template\n",
    "\n",
    "class UtilityQueryGenerator:\n",
    "    def __init__(self, llm, embedding_model, similarity_threshold=0.8):\n",
    "        self.llm = llm\n",
    "        self.embedding_model = embedding_model\n",
    "        self.similarity_threshold = similarity_threshold\n",
    "\n",
    "    def generate_queries(self, chunk: str, max_retries: int = 1,\n",
    "                          existing_graph_queries: List[str] = None,\n",
    "                          max_queries: int = 2) -> List[str]:\n",
    "\n",
    "        existing_graph_queries = existing_graph_queries or []\n",
    "\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                # Truncate chunk if too long\n",
    "                truncated_chunk = chunk[:1000] + \"...\" if len(chunk) > 1000 else chunk\n",
    "\n",
    "                formatted_prompt = create_utility_query_prompt(\n",
    "                    number=max_queries,\n",
    "                    data=truncated_chunk\n",
    "                )\n",
    "\n",
    "                response = self.llm.invoke(formatted_prompt).content\n",
    "\n",
    "                try:\n",
    "                    queries_dict = json.loads(response)\n",
    "                except json.JSONDecodeError:\n",
    "                    json_match = re.search(r'\\{.*\\}', response, re.DOTALL)\n",
    "                    if json_match:\n",
    "                        queries_dict = json.loads(json_match.group(0))\n",
    "                    else:\n",
    "                        continue\n",
    "\n",
    "                potential_queries = [\n",
    "                    queries_dict.get(f\"query_{i}\", \"\").strip()\n",
    "                    for i in range(1, max_queries + 1)\n",
    "                    if queries_dict.get(f\"query_{i}\")\n",
    "                ]\n",
    "\n",
    "                if not potential_queries:\n",
    "                    continue\n",
    "\n",
    "                filtered_queries = self.filter_queries(potential_queries, existing_graph_queries)\n",
    "\n",
    "                return filtered_queries\n",
    "\n",
    "            except Exception as e:\n",
    "                pass\n",
    "        return []\n",
    "\n",
    "    def parse_json_response(self, response):\n",
    "        \"\"\"\n",
    "        Robust JSON parsing with multiple fallback strategies\n",
    "        \"\"\"\n",
    "        # Strategy 1: Direct JSON parsing\n",
    "        try:\n",
    "            return json.loads(response)\n",
    "        except json.JSONDecodeError:\n",
    "            pass\n",
    "\n",
    "        # Strategy 2: Extract JSON from between first { and last }\n",
    "        try:\n",
    "            json_match = re.search(r'\\{.*\\}', response, re.DOTALL)\n",
    "            if json_match:\n",
    "                return json.loads(json_match.group(0))\n",
    "        except json.JSONDecodeError:\n",
    "            pass\n",
    "\n",
    "        # Strategy 3: Try to repair common JSON formatting issues\n",
    "        try:\n",
    "            cleaned_response = response.strip()\n",
    "            cleaned_response = cleaned_response.replace(\"'\", '\"')\n",
    "            cleaned_response = re.sub(r'(\\w+):', r'\"\\1\":', cleaned_response)\n",
    "            return json.loads(cleaned_response)\n",
    "        except Exception:\n",
    "            return {}\n",
    "\n",
    "    def calculate_query_similarity(self, query1: str, query2: str) -> float:\n",
    "        \"\"\"\n",
    "        Calculate cosine similarity between two queries using embeddings\n",
    "        \"\"\"\n",
    "        try:\n",
    "            emb1 = self.embedding_model.get_text_embedding(query1)\n",
    "            emb2 = self.embedding_model.get_text_embedding(query2)\n",
    "\n",
    "            dot_product = np.dot(emb1, emb2)\n",
    "            norm1 = np.linalg.norm(emb1)\n",
    "            norm2 = np.linalg.norm(emb2)\n",
    "\n",
    "            return dot_product / (norm1 * norm2)\n",
    "        except Exception as e:\n",
    "            return 0.0\n",
    "\n",
    "    def filter_queries(self, queries: List[str], existing_graph_queries: List[str]) -> List[str]:\n",
    "        filtered_queries = []\n",
    "\n",
    "        for query in queries:\n",
    "            if not query:\n",
    "                continue\n",
    "\n",
    "            # Check similarity with existing graph queries\n",
    "            is_unique = True\n",
    "            for graph_query in existing_graph_queries:\n",
    "                similarity = self.calculate_query_similarity(query, graph_query)\n",
    "                if similarity > self.similarity_threshold:\n",
    "                    is_unique = False\n",
    "                    break\n",
    "\n",
    "            # Check similarity with already filtered queries\n",
    "            if is_unique:\n",
    "                for filtered_query in filtered_queries:\n",
    "                    similarity = self.calculate_query_similarity(query, filtered_query)\n",
    "                    if similarity > self.similarity_threshold:\n",
    "                        is_unique = False\n",
    "                        break\n",
    "\n",
    "            if is_unique:\n",
    "                filtered_queries.append(query)\n",
    "\n",
    "        return filtered_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "oT3Yks2mcSVD"
   },
   "outputs": [],
   "source": [
    "def llm_response_if_memory_hit_found(query: str, chunk: str) -> Optional[str]:\n",
    "    prompt = f\"\"\"You are an expert reasoning agent tasked with answering the query from the given chunk of data.\n",
    "Follow these guidelines:\n",
    "1. Directly answer the query using ONLY the information in the provided chunk\n",
    "2. If the chunk does not contain sufficient information, respond with \"INSUFFICIENT_CONTEXT\"\n",
    "3. Be concise and precise in your response\n",
    "\n",
    "Example:\n",
    "Query: What is the capital of France?\n",
    "Chunk: France is a country in Western Europe. Its capital is Paris, known for the Eiffel Tower and rich cultural heritage.\n",
    "Answer: The capital of France is Paris\n",
    "\n",
    "Current Query: {query}\n",
    "Chunk: {chunk}\n",
    "Answer:\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = chat_llm.invoke(prompt)\n",
    "\n",
    "        cleaned_response = response.content.strip()\n",
    "\n",
    "        if cleaned_response in [\"INSUFFICIENT_CONTEXT\"]:\n",
    "            return None\n",
    "\n",
    "        return cleaned_response\n",
    "\n",
    "    except Exception as e:\n",
    "        # Error handling\n",
    "        print(f\"Error in LLM response generation: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lgJ1yyqtb60Z"
   },
   "source": [
    "#### Interleaving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "id": "b47Bt3NWDeA7"
   },
   "outputs": [],
   "source": [
    "from pathway.udfs import DiskCache, ExponentialBackoffRetryStrategy\n",
    "from pathway.xpacks.llm import embedders, llms, parsers, prompts\n",
    "from pathway.xpacks.llm.question_answering import BaseRAGQuestionAnswerer\n",
    "from pathway.xpacks.llm.vector_store import VectorStoreServer\n",
    "from pathway.xpacks.llm import embedders\n",
    "import pathway as pw\n",
    "import os\n",
    "import json\n",
    "import copy\n",
    "import logging\n",
    "import pathway.udfs as udfs\n",
    "from typing import Optional\n",
    "from pathway.xpacks.llm.llms import BaseChat\n",
    "import pathway as pw  # doctest: +SKIP\n",
    "from pathway.xpacks.llm import embedders, splitters, llms, parsers, prompts  # doctest: +SKIP\n",
    "from pathway.xpacks.llm.vector_store import VectorStoreServer  # doctest: +SKIP\n",
    "from pathway.udfs import DiskCache, ExponentialBackoffRetryStrategy  # doctest: +SKIP\n",
    "from pathway.xpacks.llm.question_answering import BaseRAGQuestionAnswerer  # doctest: +SKIP\n",
    "from pathway.xpacks.llm.servers import QASummaryRestServer # doctest: +SKIP\n",
    "import os\n",
    "import copy\n",
    "import pathway as pw\n",
    "from pathway.internals import udfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r7W-9BNGE2IS"
   },
   "source": [
    "#### Retriever Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NSeFB-mo8yrh"
   },
   "source": [
    "##### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "id": "y4kKJiRlHPqK"
   },
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "import os\n",
    "from llama_index.llms.together import TogetherLLM\n",
    "from llama_index.llms.groq import Groq\n",
    "from langchain_together import ChatTogether\n",
    "from langchain_groq import ChatGroq\n",
    "from llama_index.embeddings.jinaai import JinaEmbedding\n",
    "import unstructured_client\n",
    "import pathway\n",
    "from unstructured_client.models import operations, shared\n",
    "from pathway.xpacks.llm.vector_store import VectorStoreClient, VectorStoreServer\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# API access to llama-cloud\n",
    "os.environ[\"LLAMA_CLOUD_API_KEY\"] = \"llx-DJDcAzldLfbqaJaywoDwrLTySPWSdNI4OtSF6P561tujRRHD\"\n",
    "togetherapi = \"0d71614503955c339988e4ce8328405d78f8b758865d74df16da688b4f2e847f\"\n",
    "togetherapi1 = \"0d71614503955c339988e4ce8328405d78f8b758865d74df16da688b4f2e847f\"\n",
    "\n",
    "llm = Groq(model=\"llama3-70b-8192\", api_key = \"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\")\n",
    "llm2 = Groq(model=\"llama3-70b-8192\", api_key = \"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\")\n",
    "chat_llm = ChatTogether(model=\"meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo\", api_key = togetherapi1)\n",
    "\n",
    "chat_llm2 = ChatGroq(model=\"llama3-70b-8192\", api_key = \"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\")\n",
    "chat_llm1 = ChatGroq(\n",
    "    model=\"llama3-70b-8192\", api_key = \"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\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A_c1zf5I9R8v"
   },
   "source": [
    "##### Raptor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "id": "NqEnU40ZHSyX"
   },
   "outputs": [],
   "source": [
    "# RAPTOR\n",
    "\n",
    "\"\"\"\n",
    "Minorly tweaked from https://github.com/parthsarthi03/raptor/blob/master/raptor/cluster_tree_builder.py.\n",
    "\n",
    "Full credits to the original authors!\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import tiktoken\n",
    "import umap\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from typing import Dict, List, Optional\n",
    "import json\n",
    "import time\n",
    "from llama_index.core.schema import BaseNode\n",
    "\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "RANDOM_SEED = 224\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "\n",
    "def global_cluster_embeddings(\n",
    "    embeddings: np.ndarray,\n",
    "    dim: int,\n",
    "    n_neighbors: Optional[int] = None,\n",
    "    metric: str = \"cosine\",\n",
    ") -> np.ndarray:\n",
    "    if n_neighbors is None:\n",
    "        n_neighbors = int((len(embeddings) - 1) ** 0.5)\n",
    "    return umap.UMAP(\n",
    "        n_neighbors=n_neighbors, n_components=dim, metric=metric\n",
    "    ).fit_transform(embeddings)\n",
    "\n",
    "\n",
    "def local_cluster_embeddings(\n",
    "    embeddings: np.ndarray, dim: int, num_neighbors: int = 10, metric: str = \"cosine\"\n",
    ") -> np.ndarray:\n",
    "    return umap.UMAP(\n",
    "        n_neighbors=num_neighbors, n_components=dim, metric=metric\n",
    "    ).fit_transform(embeddings)\n",
    "\n",
    "\n",
    "def get_optimal_clusters(\n",
    "    embeddings: np.ndarray, max_clusters: int = 50, random_state: int = RANDOM_SEED\n",
    ") -> int:\n",
    "    max_clusters = min(max_clusters, len(embeddings))\n",
    "    n_clusters = np.arange(1, max_clusters)\n",
    "    bics = []\n",
    "    for n in n_clusters:\n",
    "        gm = GaussianMixture(n_components=n, random_state=random_state)\n",
    "        gm.fit(embeddings)\n",
    "        bics.append(gm.bic(embeddings))\n",
    "    return n_clusters[np.argmin(bics)]\n",
    "\n",
    "\n",
    "def GMM_cluster(embeddings: np.ndarray, threshold: float, random_state: int = 0):\n",
    "    n_clusters = get_optimal_clusters(embeddings)\n",
    "    gm = GaussianMixture(n_components=n_clusters, random_state=random_state)\n",
    "    gm.fit(embeddings)\n",
    "    probs = gm.predict_proba(embeddings)\n",
    "    labels = [np.where(prob > threshold)[0] for prob in probs]\n",
    "    return labels, n_clusters\n",
    "\n",
    "\n",
    "def perform_clustering(\n",
    "    embeddings: np.ndarray,\n",
    "    dim: int,\n",
    "    threshold: float,\n",
    ") -> List[np.ndarray]:\n",
    "    # If the number of embeddings is less than or equal to the dimension, return a list of zeros\n",
    "    # This means all nodes are in the same cluster.\n",
    "    # Otherwise, we will get an error when trying to cluster.\n",
    "    if len(embeddings) <= dim + 1:\n",
    "        return [np.array([0]) for _ in range(len(embeddings))]\n",
    "\n",
    "    reduced_embeddings_global = global_cluster_embeddings(embeddings, dim)\n",
    "    global_clusters, n_global_clusters = GMM_cluster(\n",
    "        reduced_embeddings_global, threshold\n",
    "    )\n",
    "\n",
    "    all_local_clusters = [np.array([]) for _ in range(len(embeddings))]\n",
    "    total_clusters = 0\n",
    "\n",
    "    for i in range(n_global_clusters):\n",
    "        global_cluster_embeddings_ = embeddings[\n",
    "            np.array([i in gc for gc in global_clusters])\n",
    "        ]\n",
    "\n",
    "        if len(global_cluster_embeddings_) == 0:\n",
    "            continue\n",
    "        if len(global_cluster_embeddings_) <= dim + 1:\n",
    "            local_clusters = [np.array([0]) for _ in global_cluster_embeddings_]\n",
    "            n_local_clusters = 1\n",
    "        else:\n",
    "            reduced_embeddings_local = local_cluster_embeddings(\n",
    "                global_cluster_embeddings_, dim\n",
    "            )\n",
    "            local_clusters, n_local_clusters = GMM_cluster(\n",
    "                reduced_embeddings_local, threshold\n",
    "            )\n",
    "\n",
    "        for j in range(n_local_clusters):\n",
    "            local_cluster_embeddings_ = global_cluster_embeddings_[\n",
    "                np.array([j in lc for lc in local_clusters])\n",
    "            ]\n",
    "            indices = np.where(\n",
    "                (embeddings == local_cluster_embeddings_[:, None]).all(-1)\n",
    "            )[1]\n",
    "            for idx in indices:\n",
    "                all_local_clusters[idx] = np.append(\n",
    "                    all_local_clusters[idx], j + total_clusters\n",
    "                )\n",
    "\n",
    "        total_clusters += n_local_clusters\n",
    "\n",
    "    return all_local_clusters\n",
    "\n",
    "\n",
    "def get_clusters(\n",
    "    nodes: List[BaseNode],\n",
    "    embedding_map: Dict[str, List[List[float]]],\n",
    "    max_length_in_cluster: int = 10000,  # 10k tokens max per cluster\n",
    "    tokenizer: tiktoken.Encoding = tiktoken.get_encoding(\"cl100k_base\"),\n",
    "    reduction_dimension: int = 10,\n",
    "    threshold: float = 0.1,\n",
    "    prev_total_length=None,  # to keep track of the total length of the previous clusters\n",
    ") -> List[List[BaseNode]]:\n",
    "\n",
    "    embeddings = np.array([np.array(embedding_map[node.id_]) for node in nodes])\n",
    "    clusters = perform_clustering(\n",
    "        embeddings, dim=reduction_dimension, threshold=threshold\n",
    "    )\n",
    "\n",
    "    # Initialize an empty list to store the clusters of nodes\n",
    "    node_clusters = []\n",
    "\n",
    "    # Iterate over each unique label in the clusters\n",
    "    for label in np.unique(np.concatenate(clusters)):\n",
    "        indices = [i for i, cluster in enumerate(clusters) if label in cluster]\n",
    "        cluster_nodes = [nodes[i] for i in indices]\n",
    "        if len(cluster_nodes) == 1:\n",
    "            node_clusters.append(cluster_nodes)\n",
    "            continue\n",
    "        total_length = sum([len(tokenizer.encode(node.text)) for node in cluster_nodes])\n",
    "        if total_length > max_length_in_cluster and (\n",
    "            prev_total_length is None or total_length < prev_total_length\n",
    "        ):\n",
    "            node_clusters.extend(\n",
    "                get_clusters(\n",
    "                    cluster_nodes,\n",
    "                    embedding_map,\n",
    "                    max_length_in_cluster=max_length_in_cluster,\n",
    "                    tokenizer=tokenizer,\n",
    "                    reduction_dimension=reduction_dimension,\n",
    "                    threshold=threshold,\n",
    "                    prev_total_length=total_length,\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            node_clusters.append(cluster_nodes)\n",
    "\n",
    "    return node_clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "id": "2yH5eC3Z1rNj"
   },
   "outputs": [],
   "source": [
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "import asyncio\n",
    "from enum import Enum\n",
    "\n",
    "from llama_index.core import (\n",
    "    StorageContext,\n",
    "    VectorStoreIndex,\n",
    "    get_response_synthesizer,\n",
    "    load_index_from_storage,\n",
    ")\n",
    "from pathway.xpacks.llm.vector_store import VectorStoreClient, VectorStoreServer\n",
    "from llama_index.core.base.response.schema import Response\n",
    "from llama_index.core.base.base_retriever import BaseRetriever, QueryType\n",
    "from llama_index.core.bridge.pydantic import BaseModel, Field\n",
    "from llama_index.core.embeddings import BaseEmbedding\n",
    "from llama_index.core.ingestion import run_transformations\n",
    "from llama_index.core.llama_pack.base import BaseLlamaPack\n",
    "from llama_index.core.llms.llm import LLM\n",
    "from llama_index.core.response_synthesizers import BaseSynthesizer\n",
    "from llama_index.core.schema import (\n",
    "    BaseNode,\n",
    "    NodeWithScore,\n",
    "    QueryBundle,\n",
    "    TextNode,\n",
    "    TransformComponent,\n",
    ")\n",
    "from llama_index.core.vector_stores.types import (\n",
    "    MetadataFilter,\n",
    "    MetadataFilters,\n",
    "    BasePydanticVectorStore,\n",
    ")\n",
    "from llama_index.packs.raptor.clustering import get_clusters\n",
    "\n",
    "\n",
    "DEFAULT_SUMMARY_PROMPT = (\n",
    "    \"Summarize the provided text, including as many key details as needed.\"\n",
    ")\n",
    "\n",
    "import os\n",
    "# Path to your JSONL file\n",
    "def process_json(input_file_path):\n",
    "    os.makedirs('JSON_DATA2' , exist_ok=True)\n",
    "    output_jsonl_file_path = \"JSON_DATA2/processed_data.jsonl\"\n",
    "    processed_data = []\n",
    "    with open(input_file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            data = json.loads(line.strip())\n",
    "            if \"text\" in data:\n",
    "                data[\"data\"] = data.pop(\"text\")\n",
    "            if \"metadata\" in data:\n",
    "                metadata = data[\"metadata\"]\n",
    "                # Ensure \"level\" is present, set to None if missing\n",
    "                if \"level\" not in metadata.keys():\n",
    "                    metadata[\"level\"] = -1\n",
    "                # Ensure \"parent_id\" is present, set to None if missing\n",
    "                if \"parent_id\" not in metadata.keys():\n",
    "                    metadata[\"parent_id\"] = \"nothing\"\n",
    "            data[\"level\"] = str(data[\"metadata\"][\"level\"])\n",
    "            data[\"parent_id\"] = data[\"metadata\"][\"parent_id\"]\n",
    "            data.pop(\"metadata\")\n",
    "            if \"embedding\" in data:\n",
    "                data.pop(\"embedding\")\n",
    "                data.pop('excluded_embed_metadata_keys')\n",
    "                data.pop('excluded_llm_metadata_keys')\n",
    "                data.pop('metadata_template')\n",
    "                data.pop('metadata_seperator')\n",
    "                data.pop('mimetype')\n",
    "                data.pop('start_char_idx')\n",
    "                data.pop('end_char_idx')\n",
    "                data.pop('text_template')\n",
    "                data.pop('metadata_separator')\n",
    "            processed_data.append(data)\n",
    "\n",
    "    with open(output_jsonl_file_path, 'w') as output_file:\n",
    "        for entry in processed_data:\n",
    "            output_file.write(json.dumps(entry) + '\\n')\n",
    "\n",
    "    print(f\"Processed data has been written to {output_jsonl_file_path}\")\n",
    "\n",
    "    return output_jsonl_file_path\n",
    "\n",
    "\n",
    "class QueryModes(str, Enum):\n",
    "    \"\"\"Query modes.\"\"\"\n",
    "\n",
    "    tree_traversal = \"tree_traversal\"\n",
    "    collapsed = \"collapsed\"\n",
    "\n",
    "\n",
    "class SummaryModule(BaseModel):\n",
    "    response_synthesizer: BaseSynthesizer = Field(description=\"LLM\")\n",
    "    summary_prompt: str = Field(\n",
    "        default=DEFAULT_SUMMARY_PROMPT,\n",
    "        description=\"Summary prompt.\",\n",
    "    )\n",
    "    num_workers: int = Field(\n",
    "        default=4, description=\"Number of workers to generate summaries.\"\n",
    "    )\n",
    "    show_progress: bool = Field(default=True, description=\"Show progress.\")\n",
    "\n",
    "    class Config:\n",
    "        arbitrary_types_allowed = True\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        llm: Optional[LLM] = None,\n",
    "        summary_prompt: str = DEFAULT_SUMMARY_PROMPT,\n",
    "        num_workers: int = 4,\n",
    "    ) -> None:\n",
    "        response_synthesizer = get_response_synthesizer(\n",
    "            response_mode=\"tree_summarize\", use_async=True, llm=llm\n",
    "        )\n",
    "        super().__init__(\n",
    "            response_synthesizer=response_synthesizer,\n",
    "            summary_prompt=summary_prompt,\n",
    "            num_workers=num_workers,\n",
    "        )\n",
    "\n",
    "    async def generate_summaries(\n",
    "        self, documents_per_cluster: List[List[BaseNode]]\n",
    "    ) -> List[str]:\n",
    "        \"\"\"Generate summaries of documents per cluster.\n",
    "\n",
    "        Args:\n",
    "            documents_per_cluster (List[List[BaseNode]]): List of documents per cluster\n",
    "\n",
    "        Returns:\n",
    "            List[str]: List of summary for each cluster\n",
    "        \"\"\"\n",
    "        jobs = []\n",
    "        for documents in documents_per_cluster:\n",
    "            with_scores = [NodeWithScore(node=doc, score=1.0) for doc in documents]\n",
    "            jobs.append(\n",
    "                self.response_synthesizer.asynthesize(self.summary_prompt, with_scores)\n",
    "            )\n",
    "\n",
    "        lock = asyncio.Semaphore(self.num_workers)\n",
    "        responses = []\n",
    "\n",
    "        for job in jobs:\n",
    "            async with lock:\n",
    "                responses.append(await job)\n",
    "\n",
    "        return [str(response) for response in responses]\n",
    "\n",
    "\n",
    "def generate_id_mapping_from_jsonl(jsonl_file_path: str) -> dict:\n",
    "    count = 0\n",
    "    id_mapping = {}\n",
    "    num_id_mapping = {}\n",
    "\n",
    "    with open(jsonl_file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            data = json.loads(line.strip())\n",
    "\n",
    "            id_ = data.get(\"id_\")\n",
    "            if id_ not in id_mapping:\n",
    "                num_id_mapping[id_] = count\n",
    "                id_mapping[id_] = str(count)\n",
    "                count += 1\n",
    "\n",
    "    return id_mapping , num_id_mapping\n",
    "\n",
    "def update_jsonl_with_id_mapping(input_file_path: str, output_file_path: str, id_mapping: dict , num_id_mapping : dict):\n",
    "    os.makedirs(\"JSON_DATA\" , exist_ok=True)\n",
    "    with open(input_file_path, 'r') as infile, open(output_file_path, 'w') as outfile:\n",
    "        for line in infile:\n",
    "            data = json.loads(line.strip())\n",
    "\n",
    "            if \"id_\" in data:\n",
    "                data[\"id_\"] = num_id_mapping.get(data[\"id_\"], data[\"id_\"])\n",
    "            if \"parent_id\" in data:\n",
    "                data[\"parent_id\"] = id_mapping.get(data[\"parent_id\"], data[\"parent_id\"])\n",
    "\n",
    "            outfile.write(json.dumps(data) + '\\n')\n",
    "    outfile.close()\n",
    "    time.sleep(100)\n",
    "\n",
    "\n",
    "\n",
    "class RaptorRetriever(BaseRetriever):\n",
    "    \"\"\"Raptor indexing retriever.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        documents: List[BaseNode],\n",
    "        tree_depth: int = 3,\n",
    "        similarity_top_k: int = 2,\n",
    "        llm: Optional[LLM] = None,\n",
    "        embed_model: Optional[BaseEmbedding] = None,\n",
    "        vector_store: Optional[BasePydanticVectorStore] = None,\n",
    "        transformations: Optional[List[TransformComponent]] = None,\n",
    "        summary_module: Optional[SummaryModule] = None,\n",
    "        existing_index: Optional[VectorStoreIndex] = None,\n",
    "        mode: QueryModes = \"collapsed\",\n",
    "        **kwargs: Any,\n",
    "    ) -> None:\n",
    "        \"\"\"Init params.\"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "        self.mode = mode\n",
    "        self.summary_module = summary_module or SummaryModule(llm=llm)\n",
    "        self.index = existing_index or VectorStoreIndex(\n",
    "            nodes=[],\n",
    "            storage_context=StorageContext.from_defaults(vector_store=vector_store),\n",
    "            embed_model=embed_model,\n",
    "            transformations=transformations,\n",
    "        )\n",
    "        self.tree_depth = tree_depth\n",
    "        self.similarity_top_k = similarity_top_k\n",
    "\n",
    "        if len(documents) > 0:\n",
    "            asyncio.run(self.insert(documents))\n",
    "\n",
    "        # Here we will also assign the vector store and vector client\n",
    "        self.PATHWAY_PORT = 8000\n",
    "        self.LOCAL_HOST = \"127.0.0.1\"\n",
    "\n",
    "        self.client = VectorStoreClient(\n",
    "            host=\"127.0.0.1\",\n",
    "            port=self.PATHWAY_PORT,\n",
    "            timeout=1000\n",
    "        )\n",
    "        print(\"Client Initialized\")\n",
    "\n",
    "    def _get_embeddings_per_level(self, level: int = 0) -> List[float]:\n",
    "        \"\"\"Retrieve embeddings per level in the abstraction tree.\n",
    "\n",
    "        Args:\n",
    "            level (int, optional): Target level. Defaults to 0 which stands for leaf nodes.\n",
    "\n",
    "        Returns:\n",
    "            List[float]: List of embeddings\n",
    "        \"\"\"\n",
    "        filters = MetadataFilters(filters=[MetadataFilter(\"level\", level)])\n",
    "        source_nodes = self.index.as_retriever(\n",
    "            similarity_top_k=10000, filters=filters\n",
    "        ).retrieve(\"retrieve\")\n",
    "        return [x.node for x in source_nodes]\n",
    "\n",
    "    async def insert(self, documents: List[BaseNode]) -> None:\n",
    "        \"\"\"Inserts higher levels of abstraction within the index.\n",
    "\n",
    "        Args:\n",
    "            documents (List[BaseNode]): List of documents.\n",
    "        \"\"\"\n",
    "        embed_model = self.index._embed_model\n",
    "        transformations = self.index._transformations\n",
    "\n",
    "        all_nodes_data = []\n",
    "        cur_nodes = run_transformations(documents, transformations, in_place=False)\n",
    "\n",
    "        for level in range(self.tree_depth):\n",
    "            if self._verbose:\n",
    "                print(f\"Generating embeddings for level {level}.\")\n",
    "\n",
    "            embeddings = await embed_model.aget_text_embedding_batch(\n",
    "                [node.get_content(metadata_mode=\"embed\") for node in cur_nodes]\n",
    "            )\n",
    "            assert len(embeddings) == len(cur_nodes)\n",
    "            id_to_embedding = {\n",
    "                node.id_: embedding for node, embedding in zip(cur_nodes, embeddings)\n",
    "            }\n",
    "\n",
    "            if self._verbose:\n",
    "                print(f\"Performing clustering for level {level}.\")\n",
    "\n",
    "            nodes_per_cluster = get_clusters(cur_nodes, id_to_embedding)\n",
    "\n",
    "            if self._verbose:\n",
    "                print(\n",
    "                    f\"Generating summaries for level {level} with {len(nodes_per_cluster)} clusters.\"\n",
    "                )\n",
    "            summaries_per_cluster = await self.summary_module.generate_summaries(\n",
    "                nodes_per_cluster\n",
    "            )\n",
    "\n",
    "            if self._verbose:\n",
    "                print(\n",
    "                    f\"Level {level} created summaries/clusters: {len(nodes_per_cluster)}\"\n",
    "                )\n",
    "\n",
    "            new_nodes = [\n",
    "                TextNode(\n",
    "                    text=summary,\n",
    "                    metadata={\"level\": level},\n",
    "                    excluded_embed_metadata_keys=[\"level\"],\n",
    "                    excluded_llm_metadata_keys=[\"level\"],\n",
    "                )\n",
    "                for summary in summaries_per_cluster\n",
    "            ]\n",
    "\n",
    "            nodes_with_embeddings = []\n",
    "            for cluster, summary_doc in zip(nodes_per_cluster, new_nodes):\n",
    "                for node in cluster:\n",
    "                    node.metadata[\"parent_id\"] = summary_doc.id_\n",
    "                    node.excluded_embed_metadata_keys.append(\"parent_id\")\n",
    "                    node.excluded_llm_metadata_keys.append(\"parent_id\")\n",
    "                    node.embedding = id_to_embedding[node.id_]\n",
    "                    nodes_with_embeddings.append(node)\n",
    "\n",
    "                    node_data = {key: value for key, value in vars(node).items()}\n",
    "                    node_data.pop('relationships', None)\n",
    "                    all_nodes_data.append(node_data)\n",
    "\n",
    "            self.index.insert_nodes(nodes_with_embeddings)\n",
    "            cur_nodes = new_nodes\n",
    "\n",
    "        self.index.insert_nodes(cur_nodes)\n",
    "        for node in cur_nodes:\n",
    "            node_data = {key: value for key, value in vars(node).items()}\n",
    "            node_data.pop('relationships', None)\n",
    "            all_nodes_data.append(node_data)\n",
    "\n",
    "        with open('nodes_with_embeddings.jsonl', 'w') as f:\n",
    "            for node_data in all_nodes_data:\n",
    "                json.dump(node_data, f)\n",
    "                f.write('\\n')\n",
    "\n",
    "        print(\"ONE CHECK\")\n",
    "        input_file_path = process_json('nodes_with_embeddings.jsonl')\n",
    "        input_jsonl_file_path = input_file_path\n",
    "        output_jsonl_file_path = 'JSON_DATA/processed_data.jsonl'\n",
    "        id_mapping , num_id_mapping = generate_id_mapping_from_jsonl(input_jsonl_file_path)\n",
    "        update_jsonl_with_id_mapping(input_jsonl_file_path, output_jsonl_file_path, id_mapping , num_id_mapping)\n",
    "\n",
    "    async def collapsed_retrieval(self, query_str: str) -> Response:\n",
    "        \"\"\"Query the index as a collapsed tree -- i.e. a single pool of nodes.\"\"\"\n",
    "        return self.client(query = query_str , k = 3)\n",
    "\n",
    "    async def tree_traversal_retrieval(self, query_str: str) -> Response:\n",
    "        \"\"\"Query the index as a tree, traversing the tree from the top down.\"\"\"\n",
    "        parent_ids = None\n",
    "        nodes = []\n",
    "        level = self.tree_depth - 1\n",
    "        while level >= 0:\n",
    "            if parent_ids is None:\n",
    "                level2 = str(level)\n",
    "                nodes = self.client(query_str , k = 3 , metadata_filter = f\"level == `{level2}`\")\n",
    "                if self._verbose:\n",
    "                    print(f\"Retrieved {len(nodes)} from level {level}.\")\n",
    "                parent_ids = [node['metadata']['id_'] for node in nodes]\n",
    "                if self._verbose:\n",
    "                    print(f\"Retrieved parent IDs from level {level}: {parent_ids!s}\")\n",
    "            elif parent_ids:\n",
    "                nested_nodes = []\n",
    "                for id_ in parent_ids:\n",
    "                  print(type(id_))\n",
    "                  id2 = str(id_)\n",
    "                  print(id2)\n",
    "                  docs = self.client(query = query_str , k = 3 , metadata_filter = f\"parent_id == `{id2}`\")\n",
    "                  nested_nodes.append(docs)\n",
    "                nodes = [node for nested in nested_nodes for node in nested]\n",
    "                if self._verbose:\n",
    "                    print(f\"Retrieved {len(nodes)} from parents at level {level}.\")\n",
    "                level -= 1\n",
    "                parent_ids = None\n",
    "\n",
    "        return nodes\n",
    "\n",
    "    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n",
    "        \"\"\"Retrieve nodes given query and mode.\"\"\"\n",
    "        # not used, needed for type checking\n",
    "\n",
    "    def retrieve(\n",
    "        self, query_str_or_bundle: QueryType, mode: Optional[QueryModes] = None\n",
    "    ) -> List[NodeWithScore]:\n",
    "        \"\"\"Retrieve nodes given query and mode.\"\"\"\n",
    "        if isinstance(query_str_or_bundle, QueryBundle):\n",
    "            query_str = query_str_or_bundle.query_str\n",
    "        else:\n",
    "            query_str = query_str_or_bundle\n",
    "\n",
    "        return asyncio.run(self.aretrieve(query_str, mode or self.mode))\n",
    "\n",
    "    async def aretrieve(\n",
    "        self, query_str_or_bundle: QueryType, mode: Optional[QueryModes] = None\n",
    "    ) -> List[NodeWithScore]:\n",
    "        \"\"\"Retrieve nodes given query and mode.\"\"\"\n",
    "        if isinstance(query_str_or_bundle, QueryBundle):\n",
    "            query_str = query_str_or_bundle.query_str\n",
    "        else:\n",
    "            query_str = query_str_or_bundle\n",
    "\n",
    "        mode = mode or self.mode\n",
    "        if mode == \"tree_traversal\":\n",
    "            return await self.tree_traversal_retrieval(query_str)\n",
    "        elif mode == \"collapsed\":\n",
    "            return await self.collapsed_retrieval(query_str)\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid mode: {mode}\")\n",
    "\n",
    "    def persist(self, persist_dir: str) -> None:\n",
    "        self.index.storage_context.persist(persist_dir=persist_dir)\n",
    "\n",
    "    @classmethod\n",
    "    def from_persist_dir(\n",
    "        cls: \"RaptorRetriever\",\n",
    "        persist_dir: str,\n",
    "        embed_model: Optional[BaseEmbedding] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> \"RaptorRetriever\":\n",
    "        storage_context = StorageContext.from_defaults(persist_dir=persist_dir)\n",
    "        return cls(\n",
    "            [],\n",
    "            existing_index=load_index_from_storage(\n",
    "                storage_context, embed_model=embed_model\n",
    "            ),\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "\n",
    "class RaptorPack(BaseLlamaPack):\n",
    "    \"\"\"Raptor pack.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        documents: List[BaseNode],\n",
    "        llm: Optional[LLM] = None,\n",
    "        embed_model: Optional[BaseEmbedding] = None,\n",
    "        vector_store: Optional[BasePydanticVectorStore] = None,\n",
    "        similarity_top_k: int = 2,\n",
    "        mode: QueryModes = \"collapsed\",\n",
    "        verbose: bool = True,\n",
    "        **kwargs: Any,\n",
    "    ) -> None:\n",
    "        \"\"\"Init params.\"\"\"\n",
    "        self.retriever = RaptorRetriever(\n",
    "            documents,\n",
    "            embed_model=embed_model,\n",
    "            llm=llm,\n",
    "            similarity_top_k=similarity_top_k,\n",
    "            vector_store=vector_store,\n",
    "            mode=mode,\n",
    "            verbose=verbose,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "    def get_modules(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get modules.\"\"\"\n",
    "        return {\n",
    "            \"retriever\": self.retriever,\n",
    "        }\n",
    "\n",
    "    def run(\n",
    "        self,\n",
    "        query: str,\n",
    "        mode: Optional[QueryModes] = None,\n",
    "    ) -> Any:\n",
    "        \"\"\"Run the pipeline.\"\"\"\n",
    "        return self.retriever.retrieve(query, mode=mode)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_9y1unkM-QBp"
   },
   "source": [
    "##### Retreiever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "id": "G0hNkfzfsDOn"
   },
   "outputs": [],
   "source": [
    "def table_summary(html_code):\n",
    "    client = Groq(api_key=summary_groq_api)\n",
    "    prompt_text = f\"\"\"You are an assistant tasked with summarizing tables for retrieval. \\\n",
    "    These summaries will be embedded and used to retrieve the raw table elements. \\\n",
    "    You will be given with html code of table, you have to return concise summary of table (without lossing any information , including numerical), well optimized for retrieval. Table:{html_code} Summary:\"\"\"\n",
    "\n",
    "    summary = client.chat.completions.create(\n",
    "        model=\"llama-3.1-70b-versatile\",  # Specify the model you want to use\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt_text\n",
    "            }\n",
    "        ],\n",
    "        temperature=0.3,         # Control randomness\n",
    "        top_p=0.9,               # Sampling control for nucleus sampling\n",
    "        stream=False             # Change to True if you want streaming responses\n",
    "    )\n",
    "    return summary.choices[0].message.content\n",
    "\n",
    "def image_summary(image_content):\n",
    "    client = Groq(api_key=summary_groq_api)\n",
    "    image_summary = client.chat.completions.create(\n",
    "          messages=[\n",
    "              {\n",
    "                  \"role\": \"user\",\n",
    "                  \"content\": [\n",
    "                      {\"type\": \"text\", \"text\": \"Give the detailed summary of the given image , without loosing any information\"},\n",
    "                      {\n",
    "                          \"type\": \"image_url\",\n",
    "                          \"image_url\": {\n",
    "                              \"url\": f'data:image/jpeg;base64,{image_content}',\n",
    "                          },\n",
    "                      },\n",
    "                  ],\n",
    "              }\n",
    "          ],\n",
    "          temperature = 0.7,\n",
    "          model=\"llama-3.2-90b-vision-preview\",\n",
    "      )\n",
    "    return image_summary.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "id": "q7Ccn5oK1FaE"
   },
   "outputs": [],
   "source": [
    "# Jina Embedder\n",
    "from pathway.xpacks.llm.embedders import BaseEmbedder\n",
    "\n",
    "class JinaEmbedder(BaseEmbedder):\n",
    "    \"\"\"\n",
    "    Pathway wrapper for Jina's embedding API.\n",
    "\n",
    "    Args:\n",
    "        model: model name or path for the Jina embedding model.\n",
    "        api_key: optional API key for authentication.\n",
    "        call_kwargs: additional arguments to pass during each embedding call.\n",
    "        task: task to perform with the embedding model (e.g., 'retrieval.passage').\n",
    "        **sentencetransformer_kwargs: kwargs for initializing the SentenceTransformers (if needed).\n",
    "\n",
    "    Example:\n",
    "\n",
    "    >>> import pathway as pw\n",
    "    >>> from pathway.xpacks.llm import embedders\n",
    "    >>> embedder = embedders.JinaEmbedder(model=\"your-model-name\", api_key=\"your-api-key\")\n",
    "    >>> t = pw.debug.table_from_markdown('''txt Text''')\n",
    "    >>> t.select(ret=embedder(pw.this.txt))\n",
    "    <pathway.Table schema={'ret': numpy.ndarray[typing.Any, numpy.dtype[typing.Any]]}>\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: str,\n",
    "        api_key: str = None,\n",
    "        call_kwargs: dict = {},\n",
    "        task: str = \"retrieval.passage\",\n",
    "        **sentencetransformer_kwargs\n",
    "    ):\n",
    "        from llama_index.embeddings.jinaai import JinaEmbedding\n",
    "        super().__init__()\n",
    "        self.model = JinaEmbedding(\n",
    "            api_key=api_key, model=model, task=task, **sentencetransformer_kwargs\n",
    "        )\n",
    "        self.call_kwargs = call_kwargs  # Store additional arguments for calls\n",
    "\n",
    "    def __wrapped__(self, input: str, **kwargs) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Embed the input text using the Jina model.\n",
    "\n",
    "        Args:\n",
    "            - input (str): The text to embed.\n",
    "            - kwargs: Optional additional keyword arguments for fine-tuning the embedding call.\n",
    "\n",
    "        Returns:\n",
    "            - np.ndarray: The embedding result as a NumPy array.\n",
    "        \"\"\"\n",
    "        kwargs = {**self.call_kwargs, **kwargs}\n",
    "        embedding = self.model.get_text_embedding(input, **kwargs)\n",
    "\n",
    "        # Return the embedding as a numpy array\n",
    "        return np.array(embedding)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4vLnGeKmSEYS"
   },
   "source": [
    "def retriever(path, query, top_k):\n",
    "  \"\"\"\n",
    "    Retrieve and process relevant pages from a PDF document based on a semantic query.\n",
    "\n",
    "    This function extracts text and metadata from a PDF document, constructs a FAISS-based vector index for semantic search, and processes retrieved content. It returns a `RaptorRetriever` object for performing advanced semantic queries.\n",
    "\n",
    "    Args:\n",
    "        path (str): File path to the input PDF document.\n",
    "        query (str): Semantic query to search within the document.\n",
    "        top_k (int): Number of top-k most relevant pages to retrieve.\n",
    "\n",
    "    Returns:\n",
    "        RaptorRetriever: A query engine equipped to perform semantic search over the document.\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: If the specified PDF file does not exist.\n",
    "        ValueError: If the query is empty or top_k is not a positive integer.\n",
    "\n",
    "    Example:\n",
    "        >>> query_engine = retriever('document.pdf', 'research methodology', top_k=3)\n",
    "        >>> response = query_engine.query(\"Summarize the key findings\")\n",
    "  \"\"\"\n",
    "  pdf_document = fitz.open(path)\n",
    "  output_folder = \"data\"\n",
    "\n",
    "  metadata = []\n",
    "  for page_num in range(len(pdf_document)):\n",
    "      page_metadata = {\"data\": \"\", \"page_number\" : page_num + 1}\n",
    "      page = pdf_document[page_num]\n",
    "\n",
    "      page_text = page.get_text()\n",
    "      page_metadata[\"data\"] = page_text\n",
    "\n",
    "      metadata.append(page_metadata)\n",
    "\n",
    "  pdf_document.close()\n",
    "  jina_output_folder = \"./JINA\"\n",
    "  metadata_file_path = os.path.join(jina_output_folder, \"pdf_metadata.jsonl\")\n",
    "  with open(metadata_file_path, \"w\", encoding=\"utf-8\") as json_file:\n",
    "      for row in metadata:\n",
    "          json.dump(row, json_file)\n",
    "          json_file.write(\"\\n\")\n",
    "\n",
    "  json_file.close()\n",
    "  PATHWAY_PORT = 8000\n",
    "  client = VectorStoreClient(\n",
    "    host=\"0.0.0.0\",\n",
    "    port=PATHWAY_PORT,\n",
    "    timeout=80\n",
    "  )\n",
    "  time.sleep(20)\n",
    "  docs = client(query , k = top_k)\n",
    "  gt_num = []\n",
    "  for doc in docs:\n",
    "    gt_num.append(doc['metadata']['page_number'])\n",
    "\n",
    "  print(gt_num)\n",
    "\n",
    "  writer = PdfWriter()\n",
    "  input_pdf  = PdfReader(path)\n",
    "  for i in gt_num:\n",
    "    writer.add_page(input_pdf.pages[i-1])\n",
    "\n",
    "  batch_filename = 'data/elements.pdf'\n",
    "  with open(batch_filename, 'wb') as output_file:\n",
    "      writer.write(output_file)\n",
    "\n",
    "  with open(batch_filename, \"rb\") as f:\n",
    "    data = f.read()\n",
    "    req = operations.PartitionRequest(partition_parameters=shared.PartitionParameters(files=shared.Files( content=data, file_name=batch_filename) , strategy=shared.Strategy.HI_RES, languages=['eng'], extract_image_block_types=[\"Image\"]))\n",
    "    try:\n",
    "      res = pdf_loader.general.partition(request=req)\n",
    "    except Exception as e:\n",
    "      print(e)\n",
    "\n",
    "  page_metadata={}\n",
    "  for element in res.elements:\n",
    "\n",
    "      page_num = element[\"metadata\"][\"page_number\"]\n",
    "      if page_metadata.get(f\"page_{page_num}\", None) is None:\n",
    "          page_metadata[f\"page_{page_num}\"] = \"\"\n",
    "\n",
    "      if element['type'] == 'Table':\n",
    "          html = element[\"metadata\"][\"text_as_html\"]\n",
    "          text = table_summary(html)\n",
    "          time.sleep(1)\n",
    "          page_metadata[f\"page_{page_num}\"] += f\" \\n{text}\\n\"\n",
    "\n",
    "      elif element['type'] == 'Title':\n",
    "          text = element[\"text\"]\n",
    "          page_metadata[f\"page_{page_num}\"] += f\"\\n{text}\\n\"\n",
    "      elif element['type'] == 'Image':\n",
    "          base64_reper = element[\"metadata\"][\"image_base64\"]\n",
    "          text = image_summary(base64_reper)\n",
    "          time.sleep(1)\n",
    "          page_metadata[f\"page_{page_num}\"] += f\" \\n{text}\\n\"\n",
    "\n",
    "      else :\n",
    "          text = element[\"text\"]\n",
    "\n",
    "          page_metadata[f\"page_{page_num}\"] += f\"  {text}\"\n",
    "  docs = [Document(text=content, metadata={\"page_number\": page_num})\n",
    "        for page_num, content in page_metadata.items()]\n",
    "  retriever = RaptorRetriever(docs,embed_model=text_embed_model,\n",
    "        llm=llm, similarity_top_k=1,mode=\"collapsed\",\n",
    "        transformations=[SentenceSplitter(chunk_size=1000, chunk_overlap=200)],)\n",
    "\n",
    "  return retriever, gt_num #returning the retriever object and the ground-truth page numbers list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-s87aPVl-0Vg"
   },
   "source": [
    "#### Interleaving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "id": "Dk-uPg32-5EN"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import io\n",
    "\n",
    "def capture_output(r):\n",
    "  original_stdout = sys.stdout  # Save the current stdout\n",
    "  new_stdout = io.StringIO()    # Create a new buffer for stdout\n",
    "\n",
    "  try:\n",
    "      sys.stdout = new_stdout\n",
    "      a = pw.debug.compute_and_print(r, include_id=False)\n",
    "\n",
    "      captured_output = new_stdout.getvalue()\n",
    "  finally:\n",
    "      sys.stdout = original_stdout\n",
    "\n",
    "  return captured_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "id": "ZMTBWA9R-9k5"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "host_url = \"http://0.0.0.0:8765\"\n",
    "\n",
    "def _get_request_headers():\n",
    "    request_headers = {\"accept\": \"*/*\", \"Content-Type\": \"application/json\"}\n",
    "    return request_headers\n",
    "\n",
    "def retrieve(\n",
    "        query: str,\n",
    "        k: int = 3,\n",
    "        metadata_filter: str | None = None,\n",
    "        filepath_globpattern: str | None = None,\n",
    "    ) -> list[dict]:\n",
    "        \"\"\"\n",
    "        Perform a query to the vector store and fetch results.\n",
    "\n",
    "        Args:\n",
    "            query:\n",
    "            k: number of documents to be returned\n",
    "            metadata_filter: optional string representing the metadata filtering query\n",
    "                in the JMESPath format. The search will happen only for documents\n",
    "                satisfying this filtering.\n",
    "            filepath_globpattern: optional glob pattern specifying which documents\n",
    "                will be searched for this query.\n",
    "        \"\"\"\n",
    "\n",
    "        data = {\"query\": query, \"k\": k}\n",
    "        if metadata_filter is not None:\n",
    "            data[\"metadata_filter\"] = metadata_filter\n",
    "        if filepath_globpattern is not None:\n",
    "            data[\"filepath_globpattern\"] = filepath_globpattern\n",
    "        url = host_url + \"/v1/retrieve\"\n",
    "        response = requests.post(\n",
    "            url,\n",
    "            data=json.dumps(data),\n",
    "            headers=_get_request_headers(),\n",
    "            timeout=60,\n",
    "        )\n",
    "\n",
    "        responses = response.json()\n",
    "        return sorted(responses, key=lambda x: x[\"dist\"])\n",
    "\n",
    "def pw_ai_answer(prompt : str,\n",
    "                 filters : str | None = None):\n",
    "     data = {\"prompt\": prompt}\n",
    "\n",
    "     if filters is not None and filters.strip():\n",
    "            data[\"filters\"] = filters\n",
    "     url = host_url + \"/v1/pw_ai_answer\"\n",
    "     response = requests.post(url, data=json.dumps(data),\n",
    "                             headers=_get_request_headers(),\n",
    "                             timeout=60,\n",
    "                             )\n",
    "     return response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_GXgz8lP_Pjb"
   },
   "source": [
    "##### GroqChat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "id": "yFaRzA9JD8Tn"
   },
   "outputs": [],
   "source": [
    "from llama_index.llms.groq import Groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "id": "WsS4YWG__SCe"
   },
   "outputs": [],
   "source": [
    "# Custom GroqChat Implementation\n",
    "class GroqChat(BaseChat):\n",
    "    \"\"\"Pathway wrapper for Groq Chat services.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        capacity: int | None = None,\n",
    "        retry_strategy: udfs.AsyncRetryStrategy | None = None,\n",
    "        cache_strategy: udfs.CacheStrategy | None = None,\n",
    "        model: str | None = None,\n",
    "        api_key: str | None = None,\n",
    "        **groq_kwargs,\n",
    "    ):\n",
    "\n",
    "        # Use provided API key or attempt to read from environment\n",
    "        if api_key is None:\n",
    "            api_key = os.environ.get(\"GROQ_API_KEY\")\n",
    "            if not api_key:\n",
    "                raise ValueError(\n",
    "                    \"Groq API key must be provided either as an argument or in GROQ_API_KEY environment variable\"\n",
    "                )\n",
    "\n",
    "        # Prepare executor with specified capacity and retry strategy\n",
    "        executor = udfs.async_executor(\n",
    "            capacity=capacity,\n",
    "            retry_strategy=retry_strategy,\n",
    "        )\n",
    "\n",
    "        # Initialize base class\n",
    "        super().__init__(executor=executor, cache_strategy=cache_strategy)\n",
    "\n",
    "        # Store default kwargs, including API key and model\n",
    "        self.kwargs = {\"api_key\": api_key, \"model\": model}\n",
    "        self.kwargs.update(groq_kwargs)\n",
    "\n",
    "    def __wrapped__(self, messages: list[dict] | pw.Json, **kwargs) -> str | None:\n",
    "\n",
    "        # Merge default and call-specific kwargs\n",
    "        merged_kwargs = {**self.kwargs, **kwargs}\n",
    "\n",
    "        # Decode messages if using pw.Json\n",
    "        if isinstance(messages, pw.Json):\n",
    "            messages_decoded: list[dict] = messages.as_list()\n",
    "        else:\n",
    "            messages_decoded = messages\n",
    "\n",
    "        # Log request event\n",
    "        event = {\n",
    "            \"_type\": \"groq_chat_request\",\n",
    "            \"kwargs\": copy.deepcopy(merged_kwargs),\n",
    "            \"messages\": messages_decoded,\n",
    "        }\n",
    "        # logger.info(json.dumps(event, ensure_ascii=False))\n",
    "\n",
    "        # Create Groq client\n",
    "        client = Groq(model=\"llama-3.1-70b-versatile\", api_key=merged_kwargs.pop(\"api_key\"))\n",
    "\n",
    "        # Make API call\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                messages=messages_decoded,\n",
    "                model=merged_kwargs.pop(\"model\"),\n",
    "                **merged_kwargs,\n",
    "            )\n",
    "\n",
    "            # Extract response content\n",
    "            response_text = response.choices[0].message.content\n",
    "\n",
    "            # Log response event\n",
    "            event = {\n",
    "                \"_type\": \"groq_chat_response\",\n",
    "                \"response\": response_text,\n",
    "            }\n",
    "            # logger.info(json.dumps(event, ensure_ascii=False))\n",
    "\n",
    "            return response_text\n",
    "\n",
    "        except Exception:\n",
    "            # logger.error(f\"Groq API call failed: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def __call__(self, messages: pw.ColumnExpression, **kwargs) -> pw.ColumnExpression:\n",
    "        return super().__call__(messages, **kwargs)\n",
    "\n",
    "    def _accepts_call_arg(self, arg_name: str) -> bool:\n",
    "        return arg_name in [\n",
    "            \"temperature\",\n",
    "            \"max_tokens\",\n",
    "            \"top_p\",\n",
    "            \"stream\",\n",
    "            \"stop\",\n",
    "            \"presence_penalty\",\n",
    "            \"frequency_penalty\",\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "id": "2HZP2CiB_YBb"
   },
   "outputs": [],
   "source": [
    "groq_chat = GroqChat(\n",
    "    model=\"llama-3.1-70b-versatile\",\n",
    "    capacity=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yDrnEWC9_S-X"
   },
   "source": [
    "##### Interleaved RAG Question Answerer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "id": "e-cbwqdB_A1x"
   },
   "outputs": [],
   "source": [
    "\n",
    "class InterleavedRAGQuestionAnswerer(BaseRAGQuestionAnswerer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        llm,\n",
    "        default_llm_name: str | None = None,\n",
    "        short_prompt_template: pw.UDF = prompts.prompt_short_qa,\n",
    "        long_prompt_template: pw.UDF = prompts.prompt_qa,\n",
    "        summarize_template: pw.UDF = prompts.prompt_summarize,\n",
    "        strict_prompt: bool = False,\n",
    "        interleave_depth: int = 3,  # Added interleave depth parameter\n",
    "    ) -> None:\n",
    "        self.llm = llm\n",
    "        self.strict_prompt = strict_prompt\n",
    "        self.interleave_depth = interleave_depth  # Store interleave depth\n",
    "\n",
    "    def answer_query(self, pw_ai_queries: pw.Table) -> pw.Table:\n",
    "        \"\"\"Create RAG response with adaptive retrieval, incorporating interleaving.\"\"\"\n",
    "\n",
    "        table_str = capture_output(pw_ai_queries)\n",
    "        lines = table_str.strip().split('\\n')\n",
    "\n",
    "        data_line = lines[-1]\n",
    "\n",
    "        # Split by '|' and take the first part (question column)\n",
    "        prompt = data_line.split('|')[0].strip()\n",
    "\n",
    "        answer = self.answer_interleaved_query(prompt)\n",
    "        x = {\n",
    "            'prompt' :  prompt,\n",
    "            'result' : answer\n",
    "        }\n",
    "\n",
    "        df = pd.DataFrame([x], index = [0])\n",
    "        t = pw.debug.table_from_pandas(df)\n",
    "        return t\n",
    "\n",
    "\n",
    "    def answer_interleaved_query(\n",
    "        self,\n",
    "        prompt : str,\n",
    "        # prompt: str,\n",
    "    ) -> str:\n",
    "        \"\"\"Perform interleaved retrieval and reasoning with the thought process.\"\"\"\n",
    "\n",
    "        self.agent_input = \"\"\n",
    "        finished = False\n",
    "        step_n = 0\n",
    "        answer = None\n",
    "\n",
    "        while not finished and step_n < self.interleave_depth:\n",
    "            step_n += 1\n",
    "            print(f\"\\nStep {step_n} of interleaving retrieval and reasoning\")\n",
    "            # prompt = pw_ai_queries.prompt\n",
    "            thought_prompt = THOUGHT_PROMPT.format(\n",
    "                question=prompt,\n",
    "                agent_input=self.agent_input\n",
    "            )\n",
    "            self.agent_input += thought_prompt\n",
    "            x ={\n",
    "                    'prompt': thought_prompt,\n",
    "                    'model' : 'llama3-70b-8192'\n",
    "              }\n",
    "            df = pd.DataFrame([x], index = [0])\n",
    "            print(df.head())\n",
    "            # Use LLM to decide if the next step should be retrieval or reasoning\n",
    "            t = pw.debug.table_from_pandas(df)\n",
    "\n",
    "            r = t.select(ret=self.llm(llms.prompt_chat_single_qa(t.prompt), model=t.model))\n",
    "\n",
    "            thought_response = capture_output(r)[3:]\n",
    "            print(thought_response)\n",
    "\n",
    "            # print(f\"THOUGHT RESPONSE: {thought_response}\")\n",
    "            self.agent_input+=thought_response + '\\n'\n",
    "\n",
    "            # Alternate between retrieval and reasoning\n",
    "            if \"RETRIEVAL\" in thought_response:\n",
    "                query_result = self._perform_retrieval_step(\n",
    "                    self.agent_input, thought_response[20:]\n",
    "                )\n",
    "                self.agent_input += f\"\\nOBSERVATION: {query_result}\"\n",
    "                print(f\"RAG ACTION: {query_result}\")\n",
    "\n",
    "            elif \"REASONING\" in thought_response:\n",
    "                # Perform reasoning step if the thought suggests so\n",
    "                reasoning_response = self._perform_reasoning_step(self.agent_input, thought_response[20:])\n",
    "                self.agent_input += reasoning_response\n",
    "                if \"FINAL ANSWER\" in reasoning_response:\n",
    "                  answer = reasoning_response\n",
    "                  finished=True\n",
    "                else:\n",
    "                  print(f\"REASONING: {reasoning_response}\")\n",
    "\n",
    "            # Check if we've reached a final answer\n",
    "            elif \"FINAL ANSWER\" in thought_response:\n",
    "                answer = thought_response\n",
    "                finished = True\n",
    "\n",
    "        if not answer:\n",
    "            print(\"Max iterations or steps reached without a final answer.\")\n",
    "        return answer\n",
    "\n",
    "    def _perform_retrieval_step(\n",
    "        self,\n",
    "        agent_input: str,\n",
    "        query : str\n",
    "    ) -> str:\n",
    "        \"\"\"Handle the retrieval operation during RAG.\"\"\"\n",
    "        query_result = pw_ai_answer(prompt=query)\n",
    "        a = str(query_result)\n",
    "        return a\n",
    "\n",
    "    def _perform_reasoning_step(self, agent_input: str, query:str) -> str:\n",
    "      \"\"\"Handle the reasoning operation during RAG.\"\"\"\n",
    "      reasoning_prompt = REASONING_PROMPT.format(\n",
    "          question=query,\n",
    "          agent_input=self.agent_input\n",
    "      )\n",
    "      self.agent_input += reasoning_prompt\n",
    "            # print(thought_prompt)\n",
    "\n",
    "      x ={\n",
    "        'prompt': reasoning_prompt,\n",
    "        'model' : 'llama3-70b-8192'\n",
    "      }\n",
    "\n",
    "      df = pd.DataFrame([x], index = [0])\n",
    "      print(df)\n",
    "      # Use LLM to decide if the next step should be retrieval or reasoning\n",
    "      t = pw.debug.table_from_pandas(df)\n",
    "      r = t.select(ret=self.llm(llms.prompt_chat_single_qa(t.prompt), model=t.model))\n",
    "      reasoning_response = capture_output(r)[3:]\n",
    "      return reasoning_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "id": "lTkz10JAAiIY"
   },
   "outputs": [],
   "source": [
    "# Instantiate the RAG question answerer with the required parameters\n",
    "\n",
    "rag_answerer = InterleavedRAGQuestionAnswerer(\n",
    "    llm = groq_chat,\n",
    "    strict_prompt = False,\n",
    "    interleave_depth = 15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yHjjDmtaA_WO"
   },
   "outputs": [],
   "source": [
    "# Example sample query\n",
    "sample_query = \"What is the Pathway?\"\n",
    "\n",
    "# Create a DataFrame or table to mimic input format (pw.Table)\n",
    "import pandas as pd\n",
    "pw_ai_queries = pd.DataFrame({\n",
    "    'prompt': [sample_query],\n",
    "    'filters': [{}]  # You can add specific filters if needed, or leave empty\n",
    "})\n",
    "\n",
    "t1 = pw.debug.table_from_pandas(pw_ai_queries)\n",
    "# Run the query through the answer_query method\n",
    "result = rag_answerer.answer_query(t1)\n",
    "# print(\"Answer:\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EIfo-jSzppq6"
   },
   "source": [
    "### Prompts of the Multi-Agent Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "2kYTyAtIHbxD"
   },
   "outputs": [],
   "source": [
    "CODE_AGENT_PROMPT = \"\"\"\n",
    "As an expert Python code-writing/function calling agent, you have to address a user query by decomposing it into individual steps[acc to tools provided], invoking a single function/tool call at each step until the task is fully resolved using these individual steps.\n",
    "Before calling any tool, carefully examine \"Tool Calling History\" and \"Responses\". If the query cannot yet be fully answered, break it into next subtask and call the next tool to advance each subtask.\n",
    "\n",
    "** DO NOT MAKE UNNECESSARY OR IRRELEVANT CALLS, AS IT WILL COST HUGE\n",
    "\n",
    "** `NONE` IS A SPECIAL TOOL, USED TO CREATE TOOL WHEN THERE IS NO SUITABLE TOOL AVAILABLE\n",
    "** USE RAG TO OBTAIN MORE INFORMATION OR TO ONLY GATHER EXPLICIT INFORMATION\n",
    "** USE WEBSEARCH TOOL TO OBTAIN GENERAL INFORMATION OR USE IT TO ONLY GATHER EXPLICIT INFORMATION\n",
    "** DO NOT USE RAG AGENT TO PERFORM ANY TYPE OF OPERATION ON EXISTING INFORMATION\n",
    "** IF REQUIRED TOOL IS NOT AVAILABLE THEN CALL [\"NONE\", \"NONE\", '''Reasoning'''], not the RAG AGENT, DO NOT COMPROMISE WITH THE ACCURACY.\n",
    "** IF PREVIOUS RESPONSE HAS ALREADY ANSWERED ALL THE  PARTS OF THE QUERY THEN RETURN [\"end_tool\", [\"end\"], \"All the question are answered\"], DO NOT COMPROMISE WITH THE ACCURACY.\n",
    "** BEFORE CALLING TOOL, ENSURE THAT TOOL HAS CAPABILITIES TO PERFORM THAT TASK , `USE ITS DESCRIPTION TO DETERMINE`\n",
    "** JUST STICK TO ANSWERING WHAT THE USER ASKS, DO NOT OVERDO THINGS!\n",
    "\n",
    "RULES :\n",
    "\n",
    "1. **Avoid Redundant Tool Calls**:\n",
    "   - Before calling any tool, check the \"Tool Calling History\" and \"Responses\" to avoid repeating a call for data or tasks that have already been resolved.\n",
    "   - If information has already been retrieved, use that data for subsequent tasks instead of calling the same tool again unnecessarily.\n",
    "\n",
    "2. **Independence of Tools**:\n",
    "   - Explicitly pass all relevant context and previously obtained information as arguments when invoking a tool , as All tools operate independently and lack access to prior tool calls, observations, or the user query. so\n",
    "   - Example: If Step 1 retrieves DPO data, pass that output directly to a calculation tool for further processing rather than re-calling RAG to retrieve the same data.\n",
    "   - Example: If Query asked to calculate something complex, and you have breakdown the problem into subtask, then pass all the previous relevant information to RAG\n",
    "\n",
    "3. **Completion Check**:\n",
    "   - Before calling a tool, confirm if all parts of the query have been addressed using the tool responses.\n",
    "   - If all parts are answered, return: `[\"end_tool\", [\"end\"], \"All the question are answered]`.\n",
    "\n",
    "4. **When No Suitable Tool Exists**:\n",
    "   - IF THERE IS NO SUITABLE TOOL THEN DO NOT CALL RAG AGENT\n",
    "   - If the task requires a tool not available in the tool list, or if the query cannot proceed further due to tool limitations, return: `[\"NONE\",['''NONE'''], '''Reason why no suitable tool is available''']`.\n",
    "\n",
    "5. **Output Format**:\n",
    "   - Ouput should be a single tool call\n",
    "   - For each unique step, call the relevant tool using this format:\n",
    "     ```\n",
    "     [\"tool_name\", ['''arg1''', arg2, [\"arg3\", \"arg4\"], {{\"arg5\":\"arg6\"}}, ........], '''Explain current query requirement and why this tool is best fit for it not others''']\n",
    "     ```\n",
    "   - Ensure all arguments align strictly with the tools purpose and input requirements.\n",
    "   - ENSURE ALL THE ARGUMENTS FOLLOW THE OUTPUT FORMAT\n",
    "   - MAINTAIN THE DATA TYPE OF ALL THE ARGUMENTS\n",
    "\n",
    "6. **Step-by-Step Problem Solving**:\n",
    "   - Build the solution incrementally, resolving one subtask at a time.\n",
    "   - Ensure cost efficiency by minimizing redundant tool calls and avoiding unnecessary steps.\n",
    "   - Do not jump to the final answer until all intermediate steps are complete.\n",
    "\n",
    "Example Flow:\n",
    "```\n",
    "    `SUPPOSE` initially , only the tool1 and tool2 are present ; tool1 - perform document based qa task, tool2 - perform calculation, tool3 - add two number (args :- int a, int b)\n",
    "    Query: \"What is the square root of the 2500 + company's gross margin in FY 2023? Generate a translation of the answer into German?\"\n",
    "\n",
    "    # Step 1: [\"tool1\", ['''What was the gross margin in FY 2023?'''], '''As we need gross margin first , which we have to retrieve from document so calling tool1''']\n",
    "      response The gorss margin of comapny is $2500\n",
    "\n",
    "    # Step 2: [\"tool3\", [2500, 2500], '''As now we have to add two numbers , we will call tool3 , as, we have tool2 but tool3 is buillt for adding so calling it''' ]\n",
    "      response The addition is 5000\n",
    "\n",
    "    # Step 2: [\"tool2\", ['''What is the square root of 5000?'''], '''As now we have addition of 2500 , gross margin , we now have to calculate the square roor, which is a calculation task not a norma qa task so calling tool2, we can call tool1 but ''']\n",
    "      response The square root of 2500 is 50\n",
    "\n",
    "    # Step 3: [\"NONE\", [\"NONE\"], \"There is no suffiecient tool for translating , as tool1 perform document based qa task , we can call it but is not made for it , while NONE is made for handling such task where there is no specific tool , and tool2 perform calculation and this is a translation task, so we reuire a addition tool so returning NONE\"]\n",
    "      response ADD tool3 in your tool list which can translate given text to any language provided in the query\n",
    "\n",
    "    # Step 4: [\"tool3\", ['''Translate `Square root of grass margin 2500 is 50` into German''''], '''As tool3 is specifally designed to perform translation tool''']\n",
    "      response translation\n",
    "\n",
    "    # Step 5: [\"end_tool\", '''end''', \"All the question are asnwered\"]\n",
    "\n",
    "````\n",
    "End Of Example\n",
    "\n",
    "IMPORTANT:\n",
    "- IF THERE ARE NO RELEVANT TOOLS TO FURTHER ANSWER THE QUERY OR THE TOOL LIST IS EMPTY, RETURN NONE.\n",
    "- `AVOID REPETATIVE CALLS, CAREFULLY EXAMINE THE PREVIOUS TOOL CALLING HISTORY.`\n",
    "- VALUE OF ARGUMENT MUST ALIGN WITH THE TOOL DESCRIPTION:\n",
    "   - Use only the input formats specified in the tool descriptions.\n",
    "   - Arguments must strictly match the tools purpose and input requirements .\n",
    "\n",
    "** RETURN ONLY THE SINGLE FUNCTION CALL.\n",
    "STRICTLR FOLLOW THE RULES\n",
    "STRICTLY FOLLOW THE OUTPUT FORMAT\n",
    "DO NOT RETURN ANY OTHER THING (eg. explaination) EXCEPT THE FUNCTION CALL\n",
    "RETURN END_TOOL WHEN ALL THE PARTS ARE ANSWERED , DO NOT CALL FOR ADDITIONAL INFORMATION IF NOT ASKED\n",
    "STICK TO ONLY THE TOOLS PRESENT IN `PROVIDED TOOLS LIST`. DO NOT USE ANY OTHER TOOLS OR MAKE UP OR HALLUCINATE TOOLS.\n",
    "\n",
    "Now,\n",
    "\n",
    "Query : {query},\n",
    "Tool Calling History : {scratchpad},\n",
    "Responses : {responses},\n",
    "Available tools : {tools},\n",
    "\n",
    "OUTPUT:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "TckPa6tI5dT4"
   },
   "outputs": [],
   "source": [
    "CODE_REFLEXION_PROMPT = \"\"\"\n",
    "As an expert Code Reflexion agent, your task is to analyze and resolve errors in tool calls or Python code implementations. You will refactor the given Python code and ensure the tool calls are accurate, valid, and aligned with the query requirements. If no tool can resolve the query, provide a reasoned output indicating the limitation.\n",
    "\n",
    "Inputs Provided:\n",
    "\n",
    "Query: The task or problem description.\n",
    "Previous Tool Call: Last Tool Call With Proper Exaplaination\n",
    "Error: The specific error encountered during execution.\n",
    "Available Tools: A list of tools that can be used to resolve the query.\n",
    "\n",
    "** `NONE` IS A SPECIAL TOOL, USED TO CREATE TOOL WHEN THERE IS NO SUITABLE TOOL AVAILABLE\n",
    "** IF `rag_agent IS IN TOOLS LIST ` then USE `rag_agent` TO OBTAIN MORE INFORMATION OR TO ONLY GATHER EXPLICIT INFORMATION IF PROVIDED\n",
    "** TOOL CALL ARE USED TO SOLVE SUBTASK OR WHOLE QUERY\n",
    "\n",
    "Single Function Call per Step:\n",
    " - Invoke only one function/tool call at each step, progressing incrementally until the task is fully resolved.\n",
    "\n",
    "Workflow for Error Resolution:\n",
    "\n",
    "Analyze the Problem:\n",
    "- Analyze Query And Tool Call\n",
    "\n",
    "- Review the Error:\n",
    "  - Categorize the issue, e.g.:\n",
    "     Tool not present in the list. -> IN THIS CASE CHANGE THE TOOL\n",
    "     Incorrect arguments. -> IN THIS CASE MODIFY ARGUMENTS\n",
    "     Python syntax or Logic errors. -> IN THIS CASE DECIDE ACCORDING TO THE ERROR\n",
    "\n",
    "- Analyze the Reasoning why tool has been called\n",
    "\n",
    "- Resolve the Issue:\n",
    "  - Select the Correct Tool:\n",
    "    Ensure the tool matches the task described in the tool reasoning and .\n",
    "    If no tool is suitable, provide a reason and return NONE.\n",
    "\n",
    "  - Validate Arguments:\n",
    "     Refactor the arguments to align with the tool's description.\n",
    "     ** Mainatin the same format as in previous code\n",
    "\n",
    "  - Handle Missing Tools:\n",
    "    Find other tool which can handle the subtask\n",
    "    If no tool can address the subtask, return: [\"NONE\", [\"NONE\"], \"Reason why no suitable tool is available.\"]\n",
    "\n",
    "Rules to Follow:\n",
    "\n",
    "**Step-by-Step Problem Solving:**\n",
    "   - Build the solution incrementally, resolving one subtask at a time.\n",
    "   - Do not jump to the final answer until all intermediate steps are complete.\n",
    "\n",
    "Strictly Use Available Tools:\n",
    " - Use only tools provided in the list.\n",
    " - Do not create or assume unavailable tools.\n",
    "\n",
    "Argument Accuracy:\n",
    " - Align all arguments with tool descriptions.\n",
    " - Avoid redundant or unnecessary tool calls.\n",
    "\n",
    "Output Format:\n",
    " - Return a single valid tool call in the following format: [\"tool_name\", [\"arg1\", \"arg2\", ...], \"Explanation for tool choice and alignment with the query.\"]\n",
    " - If no suitable tool exists: [\"NONE\", [\"NONE\"], \"Reason why no suitable tool is available.\"]\n",
    "\n",
    "IMPORTANT:\n",
    "- YOU MUST RETURN ONLY AN INTEGER VALUE: `0` OR `1`, with REASONING\n",
    "- DO NOT PROVIDE ANY EXPLANATION, TEXT, OR ADDITIONAL INFORMATION BEYOND THE INTEGER VALUE.\n",
    "\n",
    "RETURN ONLY THE SINGLE FUNCTION CALL.\n",
    "STRICTLY FOLLOW THE OUTPUT FORMAT SAME AS IN PREVIOUS CODE\n",
    "RETURN ONLY TOOL CALL,  DO NOT RETURN ANY OTHER THING\n",
    "STICK TO ONLY THE TOOLS PRESENT IN `Available TOOLS LIST`. DO NOT USE ANY OTHER TOOLS OR MAKE UP OR HALLUCINATE TOOLS.\n",
    "\n",
    "\n",
    "\n",
    "Query : {query},\n",
    "Previous Code (erroneous) : {agent_code},\n",
    "Error : {error},\n",
    "Available tools : {tools}\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "us5mY26QBayR"
   },
   "outputs": [],
   "source": [
    "FAILURE_DETECTION_PROMPT = \"\"\"\n",
    "As an expert failure detection agent, you will be provided with the query, your last Python code implementation, the error traceback in your last implementation, and a list of\n",
    "available tools. Your task is to review the traceback and follow the guidelines mentioned below:\n",
    "\n",
    "1. If the error traceback is an APIError  RETURN 1.\n",
    "2. If the tool generated DOES NOT EXIST in the Available Tools list given to you, RETURN 0.\n",
    "3. If the error traceback is a SilentError, RETURN 0.\n",
    "4. Else, if the traceback implies a python error, that is, the arguments have been passed wrongly, or not in the correct order or the correct data type, RETURN 0.\n",
    "\n",
    "DO NOT RETURN ANYTHING ELSE EXCEPT THE INTEGER VALUES.\n",
    "\n",
    "Now,\n",
    "\n",
    "Previous code (wrong) : {agent_code},\n",
    "Error traceback :{traceback},\n",
    "Available Tools : {tools},\n",
    "Descriptions of available tools : {descs}\n",
    "\n",
    "OUTPUT:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "D0CdvAwzaOv5"
   },
   "outputs": [],
   "source": [
    "CRITIC_AGENT_PROMPT_1 = \"\"\"\n",
    "You are an expert critic agent tasked with evaluating tool arguments. For each tool call, you will be provided with:\n",
    "- The user query.\n",
    "- The tool call with reasoning why it is called.\n",
    "- The arguments passed to the tool.\n",
    "- A history of prior tool calls and responses.\n",
    "\n",
    "Your goal is to assess whether the `arguments` are valid and relevant in the given context and the tool's function. Follow these guidelines:\n",
    "\n",
    "### RULES:\n",
    "\n",
    "- Analyze the scratchpad to understand the current progress toward solving the query.\n",
    "- Focus on the `args` list (tool arguments).\n",
    "- Check if the arguments are valid and make sense in the context of the user query.\n",
    "- Tool is called to perform the subtask of the actual query\n",
    "- PROPERLY ANALYZE THE REASONING WHY TOOL HAS BEEN CALLED\n",
    "\n",
    "- If the arguments are nonsensical or completely irrelevant and not relevant to the query and any of its subtask RETURN:\n",
    "  {{\n",
    "    \"score\": 1,\n",
    "    \"reasoning\": \"Explanation of why the arguments are invalid.\"\n",
    "  }}\n",
    "\n",
    "- Otherwise, RETURN:\n",
    "  {{\n",
    "    \"score\": 0,\n",
    "    \"reasoning\": \"Explanation of why the arguments are valid and align with the query.\"\n",
    "  }}\n",
    "\n",
    "\n",
    "## OUTPUT FORMAT (JSON-safe):\n",
    "- Ensure the output is strictly formatted as:\n",
    "  {{\n",
    "    \"score\": 0 or 1,\n",
    "    \"reasoning\": \"Your reasoning , within double inverted comma.\"\n",
    "  }}\n",
    "\n",
    "### IMPORTANT:\n",
    "- Keep reasoning concise\n",
    "- SCORE MUST BE AN INTEGER VALUE: `0` OR `1`.\n",
    "\n",
    "RETURN THE OUTPUT IN JSON FORMAT AS ABOVE\n",
    "** STRICTLY FOLLOW THE OUTPUT FORMAT\n",
    "\n",
    "Now, evaluate the following:\n",
    "Query: {query}\n",
    "Tool Call: {code_last}\n",
    "Tool Description: {desc}\n",
    "Tool Calling History: {scratchpad}\n",
    "\n",
    "OUTPUT:\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "3bmXLYcUgBzJ"
   },
   "outputs": [],
   "source": [
    "\n",
    "CRITIC_AGENT_PROMPT_2 = \"\"\"\n",
    "You are an expert critic agent tasked with evaluating tool response. For each tool call, you will be provided with:\n",
    "- Main query.\n",
    "- Tool call with reasoning why it is called.\n",
    "- Descripiton of the tool.\n",
    "- Tool Call Response\n",
    "\n",
    "Your goal is to assess whether whether the tool's response aligns with the question asked in tool call.\n",
    "QUERY HAS BEEN BREAKDOWN INTO SUBPARTS AND GIVEN TOOL HAS BEEN CALLED TO ADRESS THAT PART ONLY\n",
    "GIVEN TOOL IS CALLED TO ANSWER THE SUBPART OF THE QUERY AS EXPLAINED IN THE REASONING OF TOOL CALL\n",
    "\n",
    "### RULES:\n",
    "- Focus on the `response` (tool output).\n",
    "- Check if the output is obviously incorrect or clearly unrelated to the question asked in `TOOL ARGUMENTS`, or cannot logically be the correct response for the given TOOL ARGUMENT and context.\n",
    "  e.g., The value of 2*2 = 10000. This is clearly wrong, as 2*2 cannot possibly exceed even 10, since it is 4.\n",
    "- If the tool's intent and functionality are correctly applied but the response indicates a legitimate limitation (e.g., no data for a given query), RETURN: 0\n",
    "- If the response claims that response has been generated and saved to local device or provided location (e.g., file has been saved to your device, chart has been generated and save to your provided url ), RETURN: 0\n",
    "- If the response partially addresses the query asked in tool call or tries to answer any subparts of the query or the tool's intent, RETURN: 0\n",
    "- If the response is invalid or clearly unrelated to the tool call or cannot logically be correct, RETURN: 1\n",
    "- Otherwise, RETURN: 0\n",
    "  ```\n",
    "\n",
    "## OUTPUT FORMAT (JSON-safe):\n",
    "- Ensure the output is strictly formatted as:\n",
    "  {{\n",
    "    \"score\": 0 or 1,\n",
    "    \"reasoning\": \"Your reasoning , within double inverted comma.\"\n",
    "  }}\n",
    "\n",
    "### IMPORTANT:\n",
    "- KEEP REASONING CONCISE\n",
    "- SCORE MUST BE AN INTEGER VALUE: `0` OR `1`.\n",
    "\n",
    "STRICTLY FOLLOW THE OUTPUT FORMAT\n",
    "\n",
    "Now, evaluate the following:\n",
    "Tool Call: {code_last}\n",
    "Tool Response: {response}\n",
    "Tool Description: {desc}\n",
    "\n",
    "OUTPUT:\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "WjITBrJwU3wT"
   },
   "outputs": [],
   "source": [
    "SILENT_ERROR_REFLEXION = \"\"\"\n",
    "As an expert Python code reflexion agent, your task is to refactor the faulty tool call provided to you.\n",
    "The tool call is faulty in the sense, the arguments provided to the tool are incorrect. Using the original user query, the previous responses history, the tool call, tool description\n",
    "you are to return the refactored tool call with the correct argument values, in the exact format as the input tool call.\n",
    "\n",
    "\n",
    "Output Format:\n",
    " - Return a valid tool call in the following format AS IN TOOL CALL: [\"tool_name\", [arg1, arg2, ...], \"Explanation for tool choice and alignment with the query.\"]\n",
    "\n",
    "\n",
    "RETURN JUST THE REFACTORED TOOL CALL WITH THE CORRECT ARGUMENT VALUES IN THE EXACT FORMAT AS THE INPUT TOOL CALL.\n",
    "\n",
    "STRICTLY FOLLOW THE OUTPUT FORMAT AS IN PRVIOUS TOOL CALL\n",
    "\n",
    "Now,\n",
    "\n",
    "Tool Call : {call},\n",
    "Reason : {reason}\n",
    "User query : {query},\n",
    "Tool Calling Responses History : {scratchpad}\n",
    "\n",
    "\n",
    "OUTPUT:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "n0hDjAxxBvYy"
   },
   "outputs": [],
   "source": [
    "FINAL_RESPONSE_PROMPT = \"\"\"\n",
    "As a final response generator, you will be provided with the user query, the tools called upto this point along with their arguments, and the corresponding responses received after invoking each tool.\n",
    "Your task is to make use of these responses to generate the final answer to the user query.\n",
    "Make the final answer short and crisp. As crisp as one can get.\n",
    "\n",
    "Now,\n",
    "Query : {query},\n",
    "Tool calling history : {code},\n",
    "Responses : {responses}\n",
    "\n",
    "\n",
    "OUTPUT:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "rY5L2fMgaGg_"
   },
   "outputs": [],
   "source": [
    "CONFIDENCE_SCORE_PROMPT = \"\"\"\n",
    "You are an excellent confidence scoring prompt. You will be provided with the user query and a particular tool along with its description.\n",
    "Your task is to generate a score between 0 and 1, indicating the confidence with which you think the tool can help in answering the user question.\n",
    "\n",
    "A higher score means that the confidence that this tool is important to answering the query is high.\n",
    "\n",
    "Examples:\n",
    "```\n",
    "Query : What is the product of 11 and 12.\n",
    "Tool = Calculator\n",
    "Desc : performs arithmetic calculations\n",
    "\n",
    "Score : 0.9\n",
    "```\n",
    "\n",
    "RETURN ONLY THE SCORE\n",
    "Now,\n",
    "Query : {query},\n",
    "Tool Name : {name},\n",
    "Tool Desc : {desc}\n",
    "\n",
    "CONFIDENCE SCORE:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "wFUbk-NDE2Ib"
   },
   "outputs": [],
   "source": [
    "confidence_prompt = \"\"\"You will be provided with a series of interleaved reasoning and retrieval steps which have led to a FINAL ANSWER, that will also be provided to you.\n",
    "Based on the series of steps, you are to generate a CONFIDENCE SCORE for the FINAL ANSWER generated. The score must lie between 0 and 1 where a higher score means a greater confidence\n",
    "in the generated answer.\n",
    "\n",
    "RETURN ONLY THE SCORE.\n",
    "\n",
    "Now,\n",
    "HISTORY OF SERIES OF INTERLEAVED REASONING AND RETRIEVAL : {steps},\n",
    "FINAL ANSWER : {answer}\n",
    "\n",
    "CONFIDENCE SCORE:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "CB4nInn7E2Ib"
   },
   "outputs": [],
   "source": [
    "websearch_prompt= \"\"\"\n",
    "You are a Answer synthesizer.\n",
    "You will eb provided with the `query` and `top 3 web search result`\n",
    "Using  `ONLY web search result` and query you have to generate answer to the query\n",
    "- Do not use you existing information.\n",
    "- Output format must be string like \"Answer to the query is :- {{ans}}\"\n",
    "\n",
    "Now,\n",
    "Query :- {query}\n",
    "Web Search Result :- {response}\n",
    "\n",
    "Output :-\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "ttsDApu8BwIZ"
   },
   "outputs": [],
   "source": [
    "code_agent_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a function calling agent.\"),\n",
    "    (\"human\", CODE_AGENT_PROMPT),\n",
    "])\n",
    "\n",
    "code_reflexion_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a function call reflexion agent.\"),\n",
    "    (\"human\", CODE_REFLEXION_PROMPT),\n",
    "])\n",
    "\n",
    "critic_agent_prompt_1 = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a response critic agent.\"),\n",
    "    (\"human\", CRITIC_AGENT_PROMPT_1),\n",
    "])\n",
    "critic_agent_prompt_2 = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a response critic agent.\"),\n",
    "    (\"human\", CRITIC_AGENT_PROMPT_2),\n",
    "])\n",
    "failure_detection_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a callback failure agent.\"),\n",
    "    (\"human\", FAILURE_DETECTION_PROMPT),\n",
    "])\n",
    "\n",
    "final_response_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a final response generator.\"),\n",
    "    (\"human\", FINAL_RESPONSE_PROMPT),\n",
    "])\n",
    "confidence_score_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an excellent confidence score based critic agent.\"),\n",
    "    (\"human\", CONFIDENCE_SCORE_PROMPT)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oa36-OZY2Rsv"
   },
   "source": [
    "### Dynamic Tool Generator Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "XbaktLQJxs9M"
   },
   "outputs": [],
   "source": [
    "prompt_reflexion = \"\"\"\n",
    "You are an expert prompt generator, specializing in transforming user descriptions into highly effective prompts. Your task is to produce a prompt following a strict format (the \"meta prompt\") that will generate Python code fulfilling the user's request.\n",
    "\n",
    "*Meta Prompt Example*:\n",
    "{meta_prompt}\n",
    "\n",
    "Also,\n",
    "Here is the previous prompt : {initial_prompt}\n",
    "\n",
    "## Here is the history of all previously generated prompts.\n",
    "{history}\n",
    "\n",
    "This is the error analysis for the last prompt:\n",
    "Note that the labels here lie on a scale - [1,2,3,4,5]. The higher score means the better performance.\n",
    "ADD INSTRUCTION TO RESOLVE ERRORs AFTER ANALYZING\n",
    "{error_analysis}.\n",
    "\n",
    "Given the users task description below, and taking insights from the error analysis, replace <SHORT TASK SUMMARY> and  <insert detailed task definition here> in meta prompt to generate a prompt in the exact structure of the meta prompt that precisely captures the users requirements.\n",
    "IT IS COMPULSORY TO FOLLOW THE STRUCTURE OF META PROMPT.\n",
    "\n",
    "IMPORTANT :\n",
    "- YOU CAN REPLACE ONLY <insert detailed task definition here> AND <SHORT TASK SUMMARY>  with the suitable content\n",
    "- ADD INSTRUCTION TO RESOLVE ERRORS FROM ERROR ANALYSIS\n",
    "  eg :- x function takes only two argument\n",
    "- YOU HAVE TO STRICTLY FOLLOW FORMAT OF CODE AS EXPLAINED IN META PROMPT\n",
    "- YOU CAN NOT CHANGE OTHER CONTENT IN THE INITIAL_PROMPT\n",
    "\n",
    "*User Task Description*:\n",
    "{task_description}\n",
    "\n",
    "*Output*:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "dTM3fF5H2gcx"
   },
   "outputs": [],
   "source": [
    "edge_cases_gen = \"\"\" As an advanced language model you should create 2 highly challenging and unique samples for the task outlined below.\n",
    "These samples should be intricately designed to test the limits of the task's instructions, challenging yet relevant to the task description.\n",
    "**ENSURE THAT THESE SAMPLES DOES NOT REQUIRE ANY INPUT FROM THE USERS, ALL THE DATA MUST AVAILABLE IN THE GENERATED SAMPLE**\n",
    "\n",
    "Task Description:\n",
    "{task_description}\n",
    "\n",
    "Task Instructions:\n",
    "{instruction}\n",
    "\n",
    "A sample means a question or a query that is challenging and falls in line with the task description and instruction.\n",
    "It should be a natural language query , eg. What is ...?, Where is..? and so on.\n",
    "OUTPUT SHOULD BE A LIST OFUNIQUE SAMPLE\n",
    "\n",
    "Example :-\n",
    "OUTPUT :- ['''Question 1''',  '''Question 2''' ]\n",
    "\n",
    "### Requirements for Challenging Samples:\n",
    "Keep in mind that the samples you are generating for the task are sent to a tool, that is implemented as a single function and not a series of functions. That is forbidden for our use case. DO NOT VIOLATE THIS CONDITION.\n",
    "It IS COMPULSORY TO RETURN A LIST\n",
    "\n",
    "Generate the samples keeping these requirements in mind.\n",
    "RETURN ONLY THE SAMPLES.\n",
    "\n",
    "OUTPUT:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "gh8gqhWZ2gm-"
   },
   "outputs": [],
   "source": [
    "error_analysis = \"\"\" Assistant is a large language model designed to provide a high quality analysis for every task.\n",
    "Here is the prompt instructions that was given to the model: {prompt}\n",
    "\n",
    "An expert ranker evaluated the model's performance on the given task description.\n",
    "and rank according to the following scale: {labels}\n",
    "\n",
    "Here is a list of challenging cases for the given prompt and their rank:\n",
    "Challenging Cases: {failure_cases}\n",
    "\n",
    "Note that the ranker labels are __absolutely correct__, but the prompts (task descriptions) AND GENERATED CODE may be incorrect and need modification.\n",
    "Your task is to provide a brief analysis of the given prompt performance.\n",
    "Guidelines:\n",
    "1. The analysis should contain only the following information:\n",
    "    - A summary of the common mistakes of the PROMPT AND CODE and the ways he can be improve his generation, try to cluster the failure cases into groups and describe each group.\n",
    "2. The total length of your analysis should be less than 400 token!\n",
    "\n",
    "Analysis:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "8U6Eyj5-2kzH"
   },
   "outputs": [],
   "source": [
    "meta_prompt = \"\"\"\n",
    "You are an excellent <insert detailed task definition here>.\n",
    "Given a user input in the form of a string query , you have to ** return python programme ** to answer the ** query[provided by user] ** in context to task description. The returned value of the programme should be the answer to the query[provided by user].\n",
    "On executing the code[python programme], the value returned must be the specific answer to the provided user query, not a general implementation.\n",
    "NOTE :-\n",
    "- Use only standard python libraries.\n",
    "- Close files and clear matplotlib figure if used\n",
    "- GENRATE CODE TO ANSWER USER PROVIDED QUERY ONLY\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "4ypz9YZb2pPM"
   },
   "outputs": [],
   "source": [
    "prompt_generation_prompt = \"\"\"\n",
    "You are an expert prompt generator, specializing in transforming user descriptions into highly effective prompts. Your task is to produce a prompt following a strict format (the \"meta prompt\") that will generate Python code fulfilling the user's request.\n",
    "\n",
    "**Meta Prompt Example**:\n",
    "{meta_prompt}\n",
    "\n",
    "Given the users description below, generate a prompt in the exact structure of the meta prompt that precisely captures the users requirements.\n",
    "IT IS COMPULSORY TO FOLLOW THE ENTIRE META PROMPT .\n",
    "- **MENTION THE TASK CLEARLY and in detailed way**\n",
    "\n",
    "**User Task Description**:\n",
    "{task_description}\n",
    "\n",
    "# Example Of Task Desciption :-\n",
    "Example 1 :- You are a expert python code generator , specializing in calculator agent that can generate code with repect to user requirement\n",
    "Example 2 :- You are an expert python code generator , specializing in generating graph or chart tailored to user requirements\n",
    "\n",
    "**Output**:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "VHPtNSAx2pR4"
   },
   "outputs": [],
   "source": [
    "ranking_prmpt = \"\"\" As an excellent critic agent, you will be provided with a user query, an LLM response and the PROMPT and CODE used to generate the response.\n",
    "Your task is to rate the generated response based on the above factors, on a scale : {label_schema}, with a higher score meaning a better and more well rounded response. Provide the highest label if the output is perfect.\n",
    "\n",
    "IF RESPONSE SAYS THERE IS ERROR IN CODE THEN ANALYZE THE CODE PROPERLY\n",
    "\n",
    "STRICTLY FOLLOW THE SCALE. RETURN THE SCORE ALONG WITH YOUR SHORT CRISP REASONING FOR THE SAME.\n",
    "REASONING SHOULD BE COMPLETE , SHORT, EXPLAINING THE ERROR\n",
    "OUTPUT FORMAIT :- [INTEGER SCORE, '''REASONING''']\n",
    "\n",
    "EXAMPLE :\n",
    "\n",
    "EXAMPLE 1\n",
    "[2, '''FUNCTION REQUIRE TWO ARGUMENTS BUT ONLY ONE IS PROVIDED''']\n",
    "\n",
    "EXAMPLE 2\n",
    "[4, '''EVERYTHING IS FINE BUT TASK DESCRIPTION CAN BE IMPROVED BY ADDING DETAILS''']\n",
    "\n",
    "STRICTLY FOLLOW THE OUTPUT FORMAT\n",
    "\n",
    "Now,\n",
    "Query : {query},\n",
    "Response : {response},\n",
    "Prompt : {prompt},\n",
    "Code : {code}\n",
    "\n",
    "OUTPUT:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "yCTcfcIKA9fo"
   },
   "outputs": [],
   "source": [
    "meta_prompt_part_2 = '''\n",
    "\n",
    "Here is an example of how the markdown code must LOOK LIKE:\n",
    "Example 1:-\n",
    "```python\n",
    "<code/>\n",
    "def func_name():\n",
    "  <code/>\n",
    "  def func_2():\n",
    "    <code/>\n",
    "  <code/>\n",
    "  return ans\n",
    "ans_user_query = func_name()\n",
    "print(f\"answert to the query \" + str(ans_to_query))\n",
    "```\n",
    "Example 2 :-\n",
    "```python\n",
    "<code>\n",
    "def func_name(args):\n",
    "  <code>\n",
    "  return ans\n",
    "ans_user_query = func_name(args)\n",
    "print(f\"answert to the query \" + str(ans_to_query))\n",
    "```\n",
    "Example 3 :-\n",
    "```python\n",
    "<code>\n",
    "print(f\"answert to the query \" + str(ans_to_query))\n",
    "```\n",
    "Example 4 :-\n",
    "```python\n",
    "<code>\n",
    "def func_name():\n",
    "  <code>\n",
    "  return ans\n",
    "ans_user_query = func_name()\n",
    "print(f\"answert to the query \" + str(ans_to_query))\n",
    "```\n",
    "IT IS COMPULSORY TO HAVE ONLY ONE PRINT STATEMENT IN THE CODE STATING CODE ANSWER OF THE QUERY , EVENT IF TASK IS LIKE SAVING THE FILE OR GENERATING THE CHART THEN PRINT FILE SAVE SUCCESSFULLY OR CHART GENERATED SUCCESSFULLY.\n",
    "STRICTLY FOLLOW THE FORMAT SHOWN IN ABOVE EXAMPLES\n",
    "STRICTLY FOLLOW  THE RULES EXPLAINED ABOVE\n",
    "**USE ONLY SINGLE FUNCTION**\n",
    "CODE SHOULD STRICTYLE ANSWER ONLY TO THE USER QUERY\n",
    "\n",
    "Now,\n",
    "USER : <SHORT TASK SUMMARY> {query} #LEAVE THIS UNTOUCHED\n",
    "OUTPUT:\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "8Q0SEu0QMabY"
   },
   "outputs": [],
   "source": [
    "challenging_cases = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a challenging and edge cases generation agent\"),\n",
    "    (\"human\", edge_cases_gen),\n",
    "])\n",
    "\n",
    "ranker_instr = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a ranker agent\"),\n",
    "    (\"human\", ranking_prmpt),\n",
    "])\n",
    "\n",
    "error_analyser = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a error analysis agent\"),\n",
    "    (\"human\", error_analysis),\n",
    "])\n",
    "\n",
    "final_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a final prompt agent\"),\n",
    "    (\"human\", prompt_reflexion),\n",
    "\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "ThruoNOZMe9M"
   },
   "outputs": [],
   "source": [
    "history = []\n",
    "error_analyssis=[]\n",
    "\n",
    "def identify_challenging_examples(task_description, input_prompt):\n",
    "    '''\n",
    "    Generate challenging examples for given task_description using LLM\n",
    "\n",
    "    Args :-\n",
    "      task_description(str, required) :\n",
    "\n",
    "    Return :-\n",
    "      list : List of all challenging examples\n",
    "    '''\n",
    "    prmpt = challenging_cases.format(task_description = task_description, instruction = input_prompt)\n",
    "    challenging_examples = chat_llm.invoke(prmpt)\n",
    "    l1 = eval(challenging_examples.content)\n",
    "    print(\"Challenging Examples\")\n",
    "    print(l1)\n",
    "    return l1\n",
    "\n",
    "def annotate_challenging_examples(examples, input_prompt):\n",
    "    '''\n",
    "    Generate python code for each given example using llm and input_prompt\n",
    "\n",
    "    Args :\n",
    "      examples(list[str]) : List of challenging examples\n",
    "      input_prompt(str) : Initial prompt for code generation\n",
    "\n",
    "    Return :\n",
    "      list[dict] : List of dictionary object ,\n",
    "                  `{\"question\" : example, \"code\" : code}`\n",
    "    '''\n",
    "    annotated_examples = []\n",
    "    for example in examples:\n",
    "        prmpt = input_prompt.format(query = example)\n",
    "        annotation = chat_llm.invoke(prmpt)\n",
    "        annotated_examples.append({\"question\" : example, \"code\" : annotation.content})\n",
    "    return annotated_examples\n",
    "\n",
    "def annotate(annotations,  initial_prompt):\n",
    "    annots=[]\n",
    "    schema = [0,1,2,3,4,5]\n",
    "    for annotation in annotations:\n",
    "      a = annotation[\"code\"]\n",
    "      q = annotation[\"question\"]\n",
    "      pattern = r'```python\\n(.*?)\\n```'\n",
    "      matches = re.findall(pattern, a, re.DOTALL)\n",
    "      parsed_response = matches[-1] if matches else None\n",
    "      old_stdout = sys.stdout\n",
    "      sys.stdout = StringIO()\n",
    "      try :\n",
    "        prev_model = \" \"\n",
    "        while True :\n",
    "              try:\n",
    "                exec(parsed_response, globals())\n",
    "                annot = sys.stdout.getvalue()\n",
    "                break\n",
    "              except ModuleNotFoundError as e:\n",
    "                module_name = e.name\n",
    "                if module_name == prev_model :\n",
    "                  raise ModuleInstallError(f\"Installation failed for module '{{module_name}}': {{str(e)}}\")\n",
    "                prev_model = module_name\n",
    "                subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", module_name])\n",
    "      except Exception as e:\n",
    "        annot = traceback.format_exc()\n",
    "      sys.stdout = old_stdout\n",
    "      response = \"Generated Code : \" + parsed_response + \"\\n\" + \"Obtained output : \" + str(annot)\n",
    "      prmpt = ranker_instr.format(label_schema = schema, query = q , response =response, prompt = initial_prompt, code = a )\n",
    "      annotat = chat_llm.invoke(prmpt).content\n",
    "      prmpt_resp = eval(annotat)\n",
    "      if prmpt_resp[0] < 4 :\n",
    "          annots.append({\"Query\" : q, \"Code\": a, \"Score\" : annotat})\n",
    "    return annots\n",
    "\n",
    "def error_analysis_fun(input_prompt, annots):\n",
    "    \"\"\"\n",
    "    Anlyze error in the code and given input_prompt for given challenging example using llm\n",
    "\n",
    "    Args:\n",
    "        input_prompt (str): prompt used for generating code for challenginf examples\n",
    "        annots (list[dict]): A list of dictionaries for challenging examples where the generated code contains errors, including\n",
    "                            error analysis and scoring in the format:\n",
    "                            {\"question\": challenging example, \"code\": code, \"Score\": score}.\n",
    "\n",
    "    Returns:\n",
    "        str: Analysis of the error in the code and input_prompt\n",
    "    \"\"\"\n",
    "    labels = [0,1,2,3,4,5]\n",
    "    prmpt= error_analyser.format(prompt=input_prompt, labels = labels, failure_cases = annots)\n",
    "    analysis = chat_llm.invoke(prmpt).content\n",
    "    history.append(input_prompt)\n",
    "    error_analyssis.append(analysis)\n",
    "    return analysis\n",
    "\n",
    "def calibrate_generation_prompt(input_prompt, history, error_analysis, task_desc, meta_prompt):\n",
    "    \"\"\"\n",
    "    Refine the input prompt based on task details, history, and error analysis.\n",
    "\n",
    "    Args:\n",
    "        input_prompt (str): The prompt used for generating code for challenging examples.\n",
    "        history (list[str]): A list of previously generated prompts.\n",
    "        error_analysis (list[str]): A list of errors identified in the previously generated outputs.\n",
    "        task_desc (str): A description of the task for which the code is being generated.\n",
    "        meta_prompt (str): The base prompt used as a foundation for code generation.\n",
    "\n",
    "    Returns:\n",
    "        str: A refined prompt incorporating the task description, history, and error analysis.\n",
    "    \"\"\"\n",
    "    prmpt = chat_llm.invoke(prompt_reflexion.format(initial_prompt = input_prompt, history=history, error_analysis = error_analysis, task_description = task_desc, meta_prompt= meta_prompt))\n",
    "    return prmpt.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "j-kRkqOIbeo6"
   },
   "outputs": [],
   "source": [
    "#Dynamic Tool Generator\n",
    "def toolgenerator(task_desc, num_iter):\n",
    "  \"\"\"\n",
    "  Refine the base meta prompt iteratively for a given task description.\n",
    "\n",
    "  Args:\n",
    "      task_description (str): A brief description of the task for which the base prompt needs refinement.\n",
    "      num_iter (int): The number of iterations to refine the base prompt.\n",
    "\n",
    "  Returns:\n",
    "      str: The refined prompt after the specified number of iterations.\n",
    "  \"\"\"\n",
    "  global history , error_analysis\n",
    "  history = []\n",
    "  error_analysis=[]\n",
    "  final_prompt = \"\"\n",
    "  for i in range(num_iter):\n",
    "    initial_prompt = chat_llm.invoke(prompt_generation_prompt.format(task_description = task_desc , meta_prompt= meta_prompt))\n",
    "\n",
    "    challenging_examples = identify_challenging_examples(task_desc, initial_prompt.content)\n",
    "    initial_prompt = initial_prompt.content + meta_prompt_part_2\n",
    "    annotations = annotate_challenging_examples(challenging_examples, initial_prompt)\n",
    "    annots = annotate(annotations,  initial_prompt)\n",
    "    error_analysis_fun(initial_prompt, annots)\n",
    "    new_prompt = calibrate_generation_prompt(input_prompt= initial_prompt, history = history, error_analysis = error_analyssis, task_desc = task_desc, meta_prompt = meta_prompt + meta_prompt_part_2)\n",
    "    final_prompt = new_prompt\n",
    "  return final_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kpGhrmEibBOW"
   },
   "source": [
    "### Multi-Agent Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "-LXTG7ADHs3r"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "aylu6NTHduzF"
   },
   "outputs": [],
   "source": [
    "#Utility Functions\n",
    "\n",
    "def generate_agent_description(name, tool_desc, prompt):\n",
    "    \"\"\"\n",
    "    Generate a function description as a string.\n",
    "\n",
    "    Args:\n",
    "        name (str): The name of the function.\n",
    "        tool_desc (str): A description of what the function/tool does.\n",
    "        prompt (str or ChatPromptTemplate): The prompt used by the function.\n",
    "\n",
    "    Returns:\n",
    "        str: A formatted function definition as a string.\n",
    "    \"\"\"\n",
    "    # Ensure the prompt is properly formatted as a string\n",
    "    if isinstance(prompt, ChatPromptTemplate):\n",
    "        prompt_str = repr(prompt)  # Serialize ChatPromptTemplate as a string\n",
    "    else:\n",
    "        prompt_str = repr(prompt)\n",
    "    str1 = f\"\"\"\n",
    "def {name}(query):\n",
    "    '''\n",
    "    Name: {name}\n",
    "    Description: {tool_desc}\n",
    "    Args:\n",
    "        query (str): The query to be passed to the agent .\n",
    "                     Example: \"Args\"\n",
    "\n",
    "    Returns:\n",
    "        str: A string response\n",
    "    '''\n",
    "    annotation = chat_llm.invoke({prompt_str}.format(query=query)).content\n",
    "    pattern = r'```python\\\\n(.*?)\\\\n```'\n",
    "    matches = re.findall(pattern, annotation, re.DOTALL)\n",
    "    parsed_response = matches[-1] if matches else None\n",
    "    old_stdout = sys.stdout\n",
    "    sys.stdout = StringIO()\n",
    "    try:\n",
    "        if parsed_response:\n",
    "            prev_model = None\n",
    "            while True :\n",
    "              try:\n",
    "                exec(parsed_response, globals())\n",
    "                annot = sys.stdout.getvalue()\n",
    "                break\n",
    "              except ModuleNotFoundError as e:\n",
    "                module_name = e.name\n",
    "                if module_name == prev_model :\n",
    "                  raise ModuleInstallError(f\"Installation failed for module '{{module_name}}': {{str(e)}}\")\n",
    "                print(f\"Module '{{module_name}}' not found. Installing...\")\n",
    "                prev_model = module_name\n",
    "                subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", module_name])\n",
    "                print(f\"Module '{{module_name}}' installed successfully. Re-executing the code...\")\n",
    "        else:\n",
    "            annot = \"No valid Python code found in the response.\"\n",
    "    except Exception as e:\n",
    "        annot = traceback.format_exc()\n",
    "    finally:\n",
    "        sys.stdout = old_stdout\n",
    "    return str(annot)\n",
    "\"\"\"\n",
    "    return str1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "QCSBpOyQjrlJ"
   },
   "outputs": [],
   "source": [
    "class AgentCode:\n",
    "    def __init__(self, content):\n",
    "        self.content = content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j4ymrs2GBw4b"
   },
   "outputs": [],
   "source": [
    "class CodeandReasoningAgent():\n",
    "    def __init__(self,  tools, tools_aux, llm, tool_map, path) -> None:\n",
    "        self.llm = llm\n",
    "        self.tools = tools\n",
    "        self.reflexion_limit = 10\n",
    "        self.tool_map = tool_map\n",
    "        self.tools_aux = tools_aux\n",
    "        self.agent = None\n",
    "        self.path = path\n",
    "        self.interleave_depth = 15\n",
    "        self.vector_memory = VectorMemory.from_defaults(vector_store=None,\n",
    "          embed_model=JinaEmbedding(api_key=\"jina_63434035c44649aca2bc1294da1c74aaGQCv8Dckg-uKOLyJ819dAmPCFxYG\", model=\"jina-embeddings-v3\", task=\"retrieval.passage\",),\n",
    "          retriever_kwargs={\"similarity_top_k\": 2},)\n",
    "\n",
    "    def answer_query(self, pw_ai_queries: pw.Table) -> pw.Table:\n",
    "        \"\"\"Create RAG response with adaptive retrieval, incorporating interleaving.\"\"\"\n",
    "\n",
    "        a = capture_output(pw_ai_queries)\n",
    "        print(pw.this.prompt)\n",
    "\n",
    "        lines = a.split('\\n')\n",
    "\n",
    "        data_line = lines[1]\n",
    "        columns = data_line.split('|')\n",
    "        self.question = columns[0].strip()\n",
    "        answer = self.run(self.question)\n",
    "\n",
    "        x = {\n",
    "            'prompt' : self.question,\n",
    "            'result' : answer\n",
    "        }\n",
    "\n",
    "        df = pd.DataFrame([x], index = [0])\n",
    "        t = pw.debug.table_from_pandas(df)\n",
    "        return t\n",
    "\n",
    "    def run(self, query,  follow_up_response = None):\n",
    "        self.scratchpad = \"\"\n",
    "        flag = True\n",
    "        if follow_up_response is None:\n",
    "            # retriever_agent = retriever(self.path, query, 5)\n",
    "            # self.agent = RAGAGENT(llm=llm, embedding_dim=1024, thought_agent_prompt=thought_agent_prompt, reasoning_agent_prompt=reasoning_agent_prompt, max_steps=10, path = self.path)\n",
    "            # self.agent.engine = RetrieverQueryEngine.from_args(retriever_agent, llm=llm)\n",
    "            # self.agent.retriever = retriever_agent\n",
    "            follow_up_response = []\n",
    "            print(\"done\")\n",
    "        else:\n",
    "            self.scratchpad = f\"Information :- {follow_up_response}\"\n",
    "        self.query = query\n",
    "        self.curr_tools = deepcopy(self.tools)\n",
    "        self.curr_tools_aux = deepcopy(self.tools_aux)\n",
    "        self.responses = follow_up_response\n",
    "        self.logs = []\n",
    "        self.log_tools = []\n",
    "        self.query = query\n",
    "        self.runs= 0\n",
    "\n",
    "        print(\"Enhanced Query : \" + self.query)\n",
    "        print(self.responses)\n",
    "        while (flag or self.responses == [] or self.responses[-1] != \"end\"):\n",
    "            flag = False\n",
    "            agent_code, func_response = self.build_code()\n",
    "            if agent_code == None and func_response == None:\n",
    "                return None\n",
    "            self.responses.append(func_response)\n",
    "            self.scratchpad += '\\n' + \"Tool Call : \" + str(agent_code) + \",\" + \" Response : \" + str(func_response)\n",
    "            self.runs+=1\n",
    "        prompt = FINAL_RESPONSE_PROMPT.format(query = self.query, code = self.scratchpad, responses = self.responses)\n",
    "        x = {\n",
    "                'prompt': prompt,\n",
    "                'model' : 'llama-3.1-70b-versatile'\n",
    "          }\n",
    "        df = pd.DataFrame([x], index = [0])\n",
    "        print(df)\n",
    "        # Use LLM to decide if the next step should be retrieval or reasoning\n",
    "        t = pw.debug.table_from_pandas(df)\n",
    "        r = t.select(ret=self.llm(pw.xpacks.llm.llms.prompt_chat_single_qa(t.prompt), model=t.model))\n",
    "        final_response = capture_output(r)[3:]\n",
    "        # return reasoning_response\n",
    "        self.vector_memory.put(ChatMessage.from_str(final_response, \"user\"))\n",
    "\n",
    "        print(\"FINAL ANSWER : \", final_response)\n",
    "\n",
    "        feedback = input(\"That's that, so are there any follow up questions? \")\n",
    "        if feedback.lower() == \"no\":\n",
    "          return final_response, self.logs, self.log_tools\n",
    "        else:\n",
    "          query = input(\"Kindly enter the follow up question : \")\n",
    "          facts = self.vector_memory.get(query)\n",
    "          follow_responses = []\n",
    "          for i in range(len(facts)):\n",
    "            follow_responses.append(facts[i].content)\n",
    "          sub_ans = self.run(query, follow_responses)\n",
    "          return sub_ans, self.logs, self.log_tools\n",
    "\n",
    "    def api_reflexion(self, agent_code):\n",
    "        if(agent_code == \"NONE\" or self.curr_tools==[]):\n",
    "          while True:\n",
    "            user_input = input(\"Would you like to provide Python code with args in the docstring? (yes/no): \").strip().lower()\n",
    "            if user_input == \"yes\":\n",
    "                print(\"Please provide the proper Python code with arguments described in the docstring:\")\n",
    "                user_code = input(\"Enter your Python code:\\n\")\n",
    "                try:\n",
    "                    function_name = re.search(r\"def (\\w+)\\(\", user_code).group(1)\n",
    "                    if function_name in self.tool_map.keys():\n",
    "                      print(f\"The function '{function_name}' already exists. Re-enter your code , with different function name\")\n",
    "                      continue\n",
    "                    exec(user_code, globals())\n",
    "                    sys.stdout = out\n",
    "\n",
    "                    func = globals()[function_name]\n",
    "                    if func.__doc__ == None:\n",
    "                      print(\"Provide doc string also\")\n",
    "                      continue\n",
    "\n",
    "                    doc = f\"{func.__doc__} + This function takes the {len(list(inspect.signature(func).parameters.keys()))} arguments :  {str(list(inspect.signature(func).parameters.keys()))}\"\n",
    "\n",
    "                    self.tool_map[function_name] = func\n",
    "                    self.tools.append(doc)\n",
    "                    self.tools_aux.append(function_name)\n",
    "\n",
    "                    self.curr_tools.append(doc)\n",
    "                    self.curr_tools_aux.append(function_name)\n",
    "\n",
    "                    print(\"User-provided code accepted.\")\n",
    "                    agent_code, func_response = self.build_code()\n",
    "                    return agent_code, func_response\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error in provided code: {e}. Please try again.\")\n",
    "            elif user_input == \"no\":\n",
    "\n",
    "              print(\"System will attempt to generate Python code.\")\n",
    "              user_desc = input(\"Enter the precise description of the tool needed for the task.\")\n",
    "              tool_name = input(\"Also select a name for this tool (suggest cool names please) : \")\n",
    "              while tool_name in tool_map.keys():\n",
    "                print(f\"The function '{tool_name}' already exists. Re-enter your code , with different function name\")\n",
    "                tool_name = input(\"Select a new name for this tool (suggest cool names please) : \")\n",
    "              prompt = autoprompt(user_desc, 1)\n",
    "              sys.stdout = out\n",
    "              print(prompt)\n",
    "              # pmt = ChatPromptTemplate.from_messages([\n",
    "              #     (\"system\", \"You are a helpful assistant.\"),\n",
    "              #     (\"human\", prompt)\n",
    "              # ])\n",
    "              a = generate_agent_description(tool_name, user_desc, prompt)\n",
    "              print(a)\n",
    "              if \"python\" in a:\n",
    "                a = a[10:]\n",
    "                a = a[:len(a)-4]\n",
    "              # pattern = re.compile(r\"```python(.*?)```\")\n",
    "              # match = pattern.match(a)\n",
    "              # if match is not None:\n",
    "              #   a = match(1)\n",
    "              print(a)\n",
    "              exec(a, globals())\n",
    "              function_name = tool_name\n",
    "\n",
    "              new_func = globals()[function_name]\n",
    "              new_docs = new_func.__doc__\n",
    "              new_tool = FunctionTool.from_defaults(fn=new_func)\n",
    "              sys.stdout = out\n",
    "              new_docs = f\"{new_func.__doc__} + This function takes the {len(list(inspect.signature(new_func).parameters.keys()))} arguments :  {str(list(inspect.signature(new_func).parameters.keys()))}\"\n",
    "              self.tools.append(new_docs)\n",
    "              self.tools_aux.append(function_name)\n",
    "\n",
    "              self.curr_tools.append(new_docs)\n",
    "              self.curr_tools_aux.append(function_name)\n",
    "              self.tool_map[function_name] = new_func\n",
    "\n",
    "              agent_code, func_response = self.build_code()\n",
    "              sys.stdout = out\n",
    "              return agent_code, func_response\n",
    "            else:\n",
    "                print(\"Enter only Yes/No\")\n",
    "                continue\n",
    "        else:\n",
    "            func_name, args_list = eval(agent_code)[0], eval(agent_code)[1]\n",
    "            print(\"Reflextion\")\n",
    "            print(func_name, args_list)\n",
    "            pos = self.curr_tools_aux.index(func_name)\n",
    "            print(pos)\n",
    "            self.curr_tools_aux.remove(func_name)\n",
    "            self.curr_tools.pop(pos)\n",
    "            agent_code, func_response = self.build_code()\n",
    "            return agent_code, func_response\n",
    "        print(\"big error\")\n",
    "        return None, None\n",
    "\n",
    "    def code_reflexion(self, agent_code, error):\n",
    "        count = 0\n",
    "        agent_code = agent_code\n",
    "        error = error\n",
    "\n",
    "        while count < self.reflexion_limit:\n",
    "            too = [{self.curr_tools[i] : self.curr_tools_aux[i]} for i in range(len(self.curr_tools))]\n",
    "            prompt = CODE_REFLEXION_PROMPT.format(query = self.query, error= error, tools = too, agent_code = agent_code)\n",
    "            x ={\n",
    "                    'prompt': prompt,\n",
    "                    'model' : 'llama-3.1-70b-versatile'\n",
    "              }\n",
    "\n",
    "            df = pd.DataFrame([x], index = [0])\n",
    "            print(df)\n",
    "            # Use LLM to decide if the next step should be retrieval or reasoning\n",
    "            t = pw.debug.table_from_pandas(df)\n",
    "            r = t.select(ret=self.llm(pw.xpacks.llm.llms.prompt_chat_single_qa(t.prompt), model=t.model))\n",
    "            agent_code = capture_output(r)[3:]\n",
    "            self.logs.append({\"Reflextion\" : agent_code})\n",
    "            print(\"-----\")\n",
    "            print(\"Reflexion :=\" + agent_code)\n",
    "            print(\"-----\")\n",
    "            try:\n",
    "                if agent_code is None or agent_code.lower() == \"none\" or eval(agent_code)[0].lower() == \"none\":\n",
    "                  agent_code = \"NONE\"\n",
    "                  agent_code, func_response = self.api_reflexion(agent_code)\n",
    "                  return agent_code, func_response\n",
    "                func_name, args_list = eval(agent_code)[0], eval(agent_code)[1]\n",
    "                self.log_tools.append(func_name)\n",
    "                try :\n",
    "                  tool_call_reason = eval(agent_code)[2]\n",
    "                except :\n",
    "                  tool_call_reason = \"Reason is not Provided\"\n",
    "\n",
    "                if func_name is None or func_name.lower() == \"none\":\n",
    "                  agent_code = \"NONE\"\n",
    "                  agent_code, func_response = self.api_reflexion(agent_code)\n",
    "                  return agent_code, func_response\n",
    "\n",
    "\n",
    "                if func_name not in self.curr_tools_aux:\n",
    "                   raise ToolError(f\"Incorrect tool '{func_name}' is called. It is not in the tool list. Try a different one.\")\n",
    "\n",
    "                pos = self.curr_tools_aux.index(func_name)\n",
    "                func_desc = self.curr_tools[pos]\n",
    "\n",
    "                critics = self.critic_agent(f'''{{\"tool_name\" : {func_name}, \"args\" : {args_list} , \"reason\" : {tool_call_reason}}}''',  func_desc , None, self.scratchpad)\n",
    "                print(critics)\n",
    "\n",
    "                if(eval(critics)[\"score\"] == 1):\n",
    "                  agent_code = self.silent_reflexion(agent_code, reason = eval(critics)[\"reasoning\"])\n",
    "                  print(\"Refactored Tool Call : \" + agent_code)\n",
    "                  func_name, args_list = eval(agent_code)[0], eval(agent_code)[1]\n",
    "                  self.logs.append({\"Silet Reflexion\" : f\"Function Name :- {func_name}, Args List :- {args_list}\"})\n",
    "\n",
    "                print(\"Safe arguments\")\n",
    "                if func_name == 'rag_agent':\n",
    "                  query = args_list\n",
    "                  func_response = self.answer_interleaved_query(self.question)\n",
    "\n",
    "                else:\n",
    "                  # Proceed with normal tool execution if not a memory hit\n",
    "                  func_response = self.tool_map[func_name](args_list)\n",
    "                critics = self.critic_agent(f'''{{\"tool_name\" : {func_name}, \"arguments\" : {args_list} , \"reason\" : {tool_call_reason}}}''',  func_desc, func_response, self.scratchpad)\n",
    "                print(critics)\n",
    "\n",
    "                if(eval(critics)[\"score\"] == 1):\n",
    "                  print(\"Faulty API.\")\n",
    "                  agent_code, func_response = self.api_reflexion(agent_code)\n",
    "                else :\n",
    "                  return agent_code, func_response\n",
    "            except Exception as e:\n",
    "                sys.stdout = out\n",
    "                error_message = traceback.format_exc()\n",
    "                error = error_message\n",
    "                print(error_message)\n",
    "                failure = self.detect_failure(agent_code, error_message)\n",
    "                if eval(failure) == 1:\n",
    "                    return agent_code, None\n",
    "                else:\n",
    "                    count += 1\n",
    "\n",
    "        print(\"code reflection limit reached\")\n",
    "        return None, None\n",
    "\n",
    "\n",
    "    def parse_function(self, func_call):\n",
    "        pattern = re.compile(r'(\\w+)\\s*\\((.*)\\)')\n",
    "        match = pattern.match(func_call)\n",
    "        if not match:\n",
    "            raise ValueError(f\"Invalid function call string: {func_call}\")\n",
    "\n",
    "        func_name = match.group(1)\n",
    "        args_str = match.group(2)\n",
    "\n",
    "        if(args_str == ''):\n",
    "          return func_name, ''\n",
    "        print(args_str)\n",
    "\n",
    "        str1 = eval(args_str)\n",
    "\n",
    "        if not isinstance(str1,tuple):\n",
    "          return func_name, [str1]\n",
    "\n",
    "        return func_name,str1\n",
    "\n",
    "    #Code and Reasoning Agent\n",
    "\n",
    "    def build_code(self):\n",
    "      too = [{self.curr_tools[i] : self.curr_tools_aux[i]} for i in range(len(self.curr_tools))]\n",
    "      prompt = CODE_AGENT_PROMPT.format(query = self.query, tools = too, scratchpad = self.scratchpad, responses = self.responses)\n",
    "      x ={\n",
    "              'prompt': prompt,\n",
    "              'model' : 'llama-3.1-70b-versatile'\n",
    "        }\n",
    "\n",
    "      df = pd.DataFrame([x], index = [0])\n",
    "      print(df)\n",
    "      # Use LLM to decide if the next step should be retrieval or reasoning\n",
    "      t = pw.debug.table_from_pandas(df)\n",
    "      r = t.select(ret=self.llm(pw.xpacks.llm.llms.prompt_chat_single_qa(t.prompt), model=t.model))\n",
    "      agent_code = capture_output(r)[3:]\n",
    "      self.logs.append({\"Build Code\" : agent_code})\n",
    "\n",
    "      print(\"-----\")\n",
    "      print(\"Tool call : \" + agent_code)\n",
    "      print(\"-----\")\n",
    "\n",
    "      if agent_code.lower() == \"end_tool\" or eval(agent_code)[0].lower() == \"end_tool\":\n",
    "          return agent_code, \"end\"\n",
    "      try:\n",
    "        if agent_code is None or agent_code.lower() == \"none\" or eval(agent_code)[0].lower() == \"none\":\n",
    "          agent_code = \"NONE\"\n",
    "          agent_code, func_response = self.api_reflexion(agent_code)\n",
    "          return agent_code, func_response\n",
    "\n",
    "        func_name, args_list = eval(agent_code)[0], eval(agent_code)[1]\n",
    "        self.log_tools.append(func_name)\n",
    "\n",
    "        try :\n",
    "          tool_call_reason = eval(agent_code)[2]\n",
    "        except :\n",
    "          tool_call_reason = \"Reason is not Provided\"\n",
    "\n",
    "        if func_name is None or func_name.lower() == \"none\":\n",
    "          agent_code = \"NONE\"\n",
    "          agent_code, func_response = self.api_reflexion(agent_code)\n",
    "          return agent_code, func_response\n",
    "\n",
    "        if func_name not in self.curr_tools_aux:\n",
    "            raise ToolError(f\"Incorrect tool '{func_name}' is called. It is not in the tool list. Try a different one.\")\n",
    "\n",
    "\n",
    "        pos = self.curr_tools_aux.index(func_name)\n",
    "        func_desc = self.curr_tools[pos]\n",
    "\n",
    "        critics = self.critic_agent(f'''{{\"tool_name\" : {func_name}, \"argument\" : {args_list} , \"reason\" : {tool_call_reason}}}''',  func_desc, None, self.scratchpad)\n",
    "        print(\"PLEASE SEE : \" + critics)\n",
    "\n",
    "        if(eval(critics)[\"score\"] == 1):\n",
    "          agent_code = self.silent_reflexion(agent_code, reason = eval(critics)[\"reasoning\"])\n",
    "          print(\"Refactored Tool Call : \" + agent_code)\n",
    "          func_name, args_list = eval(agent_code)[0], eval(agent_code)[1]\n",
    "          self.logs.append({\"Silet Reflexion\" : agent_code})\n",
    "\n",
    "        print(f\"Func Name {func_name}, Args List {args_list}\")\n",
    "        if func_name == 'rag_agent':\n",
    "            func_response = self.answer_interleaved_query(self.question)\n",
    "        else:\n",
    "          # Proceed with normal tool execution if not a memory hit\n",
    "          func_response = self.tool_map[func_name](args_list)\n",
    "        print(func_response)\n",
    "\n",
    "        critics = self.critic_agent(f'''{{\"tool_name\" : {func_name}, \"argument\" : {args_list} , \"reason\" : {tool_call_reason}}}''',  func_desc,  func_response, self.scratchpad)\n",
    "        print(critics)\n",
    "        if(eval(critics)[\"score\"] == 1):\n",
    "          print(\"Faulty API.\")\n",
    "          agent_code, func_response = self.api_reflexion(agent_code)\n",
    "\n",
    "      except Exception as e:\n",
    "          sys.stdout = out\n",
    "          self.runs+=1\n",
    "          error_message = traceback.format_exc()\n",
    "          print(error_message)\n",
    "          failure = self.detect_failure(agent_code, error_message)\n",
    "          a = eval(failure)\n",
    "          if a == 1:\n",
    "              print(\"API Failure\")\n",
    "              agent_code, func_response = self.api_reflexion(agent_code)\n",
    "          else:\n",
    "              print(\"Python Error\")\n",
    "              agent_code, func_response = self.code_reflexion(agent_code, e)\n",
    "              if agent_code != None and func_response == None:\n",
    "                  # API Failure\n",
    "                  agent_code, func_response = self.api_reflexion(agent_code)\n",
    "\n",
    "      return agent_code, str(func_response)\n",
    "\n",
    "    def detect_failure(self, agent_code, callback):\n",
    "        prompt = FAILURE_DETECTION_PROMPT.format(agent_code = agent_code, traceback = callback, tools = self.curr_tools, descs = self.curr_tools_aux)\n",
    "        x ={\n",
    "                'prompt': prompt,\n",
    "                'model' : 'llama-3.1-70b-versatile'\n",
    "          }\n",
    "\n",
    "        df = pd.DataFrame([x], index = [0])\n",
    "        print(df)\n",
    "        # Use LLM to decide if the next step should be retrieval or reasoning\n",
    "        t = pw.debug.table_from_pandas(df)\n",
    "        r = t.select(ret=self.llm(pw.xpacks.llm.llms.prompt_chat_single_qa(t.prompt), model=t.model))\n",
    "        detection = capture_output(r)[3:]\n",
    "        return detection\n",
    "\n",
    "    def critic_agent(self, agent_code,  desc, func_response, scratchpad):\n",
    "      if func_response == None:\n",
    "        prompt = CRITIC_AGENT_PROMPT_1.format(query = self.query, code_last = agent_code, args = desc, scratchpad = self.scratchpad)\n",
    "        x ={\n",
    "                'prompt': prompt,\n",
    "                'model' : 'llama-3.1-70b-versatile\t'\n",
    "          }\n",
    "\n",
    "        df = pd.DataFrame([x], index = [0])\n",
    "        print(df)\n",
    "        # Use LLM to decide if the next step should be retrieval or reasoning\n",
    "        t = pw.debug.table_from_pandas(df)\n",
    "        r = t.select(ret=self.llm(pw.xpacks.llm.llms.prompt_chat_single_qa(t.prompt), model=t.model))\n",
    "        response = capture_output(r)[3:]\n",
    "      else :\n",
    "        prompt = CRITIC_AGENT_PROMPT_2.format(query = self.query, code_last = agent_code, response = func_response, args = desc, scratchpad = self.scratchpad)\n",
    "        x ={\n",
    "                'prompt': prompt,\n",
    "                'model' : 'llama-3.1-70b-versatile'\n",
    "          }\n",
    "\n",
    "        df = pd.DataFrame([x], index = [0])\n",
    "        print(df)\n",
    "        # Use LLM to decide if the next step should be retrieval or reasoning\n",
    "        t = pw.debug.table_from_pandas(df)\n",
    "        r = t.select(ret=self.llm(pw.xpacks.llm.llms.prompt_chat_single_qa(t.prompt), model=t.model))\n",
    "        response = capture_output(r)[3:]\n",
    "      return response\n",
    "\n",
    "    def silent_reflexion(self, code, reason):\n",
    "        prompt = SILENT_ERROR_REFLEXION.format(call = code, query = self.query, scratchpad = self.scratchpad)\n",
    "        x ={\n",
    "                'prompt': prompt,\n",
    "                'model' : 'llama-3.1-70b-versatile'\n",
    "          }\n",
    "\n",
    "        df = pd.DataFrame([x], index = [0])\n",
    "        print(df)\n",
    "        # Use LLM to decide if the next step should be retrieval or reasoning\n",
    "        t = pw.debug.table_from_pandas(df)\n",
    "        r = t.select(ret=self.llm(pw.xpacks.llm.llms.prompt_chat_single_qa(t.prompt), model=t.model))\n",
    "        response = capture_output(r)[3:]\n",
    "        return response\n",
    "\n",
    "    def add_tool(self, tool_desc):\n",
    "        print(\"Cool. input your function below\")\n",
    "        agent_code =  input(\"Enter the precise description of the tool needed for the task. Make sure you follow the docstring formatting as well.\")\n",
    "\n",
    "        pattern = r'python\\n(.*?)\\n'\n",
    "        matches = re.findall(pattern, agent_code, re.DOTALL)\n",
    "        parsed_response = matches[-1] if matches else None\n",
    "\n",
    "        exec(parsed_response, globals())\n",
    "\n",
    "        function_name = parsed_response.split('def ')[1].split('(')[0]\n",
    "        new_func = globals()[function_name] \n",
    "        new_docs = new_func._doc_\n",
    "        new_tool = FunctionTool.from_defaults(fn=new_func)\n",
    "\n",
    "        self.tools.append(new_docs)\n",
    "        self.tools_aux.append(function_name)\n",
    "\n",
    "        self.curr_tools.append(new_docs)\n",
    "        self.curr_tools_aux.append(function_name)\n",
    "        self.tool_map[function_name] = new_func\n",
    "\n",
    "        print(\"Tool added successfully.\")\n",
    "        agent_code, func_response = self.build_code()\n",
    "        return agent_code, func_response\n",
    "        pass\n",
    "\n",
    "    def answer_interleaved_query(\n",
    "        self,\n",
    "        prompt : str,\n",
    "        # prompt: str,\n",
    "    ) -> str:\n",
    "        \"\"\"Perform interleaved retrieval and reasoning with the thought process.\"\"\"\n",
    "\n",
    "        self.agent_input = \"\"\n",
    "        finished = False\n",
    "        step_n = 0\n",
    "        answer = None\n",
    "\n",
    "        while not finished and step_n < self.interleave_depth:\n",
    "            step_n += 1\n",
    "            print(f\"\\nStep {step_n} of interleaving retrieval and reasoning\")\n",
    "            # prompt = pw_ai_queries.prompt\n",
    "            thought_prompt = THOUGHT_PROMPT.format(\n",
    "                question=prompt,\n",
    "                agent_input=self.agent_input\n",
    "            )\n",
    "            # self.agent_input += thought_prompt\n",
    "            x ={\n",
    "                    'prompt': thought_prompt,\n",
    "                    'model' : 'llama-3.1-70b-versatile'\n",
    "              }\n",
    "            df = pd.DataFrame([x], index = [0])\n",
    "            print(df.head())\n",
    "            # Use LLM to decide if the next step should be retrieval or reasoning\n",
    "            t = pw.debug.table_from_pandas(df)\n",
    "            r = t.select(ret=self.llm(llms.prompt_chat_single_qa(t.prompt), model=t.model))\n",
    "\n",
    "            thought_response = capture_output(r)[3:]\n",
    "            print(thought_response)\n",
    "\n",
    "            # print(f\"THOUGHT RESPONSE: {thought_response}\")\n",
    "            self.agent_input+=thought_response + '\\n'\n",
    "\n",
    "            # Alternate between retrieval and reasoning\n",
    "            if \"RETRIEVAL\" in thought_response:\n",
    "                query_result = self._perform_retrieval_step(\n",
    "                    self.agent_input, thought_response[20:]\n",
    "                )\n",
    "                self.agent_input += f\"\\nOBSERVATION: {query_result}\"\n",
    "                print(f\"RAG ACTION: {query_result}\")\n",
    "\n",
    "            elif \"REASONING\" in thought_response:\n",
    "                # Perform reasoning step if the thought suggests so\n",
    "                reasoning_response = self._perform_reasoning_step(self.agent_input, thought_response[20:])\n",
    "                self.agent_input += reasoning_response\n",
    "                if \"FINAL ANSWER\" in reasoning_response:\n",
    "                  answer = reasoning_response\n",
    "                  finished=True\n",
    "                else:\n",
    "                  print(f\"REASONING: {reasoning_response}\")\n",
    "\n",
    "            # Check if we've reached a final answer\n",
    "            elif \"FINAL ANSWER\" in thought_response:\n",
    "                answer = thought_response\n",
    "                finished = True\n",
    "\n",
    "        if not answer:\n",
    "            print(\"Max iterations or steps reached without a final answer.\")\n",
    "        return answer\n",
    "\n",
    "    def _perform_retrieval_step(\n",
    "        self,\n",
    "        agent_input: str,\n",
    "        query : str\n",
    "    ) -> str:\n",
    "        \"\"\"Handle the retrieval operation during RAG.\"\"\"\n",
    "        print(query)\n",
    "        print(type(query))\n",
    "        query_result = pw_ai_answer(prompt=query)\n",
    "        a = str(query_result)\n",
    "        return a\n",
    "\n",
    "    def _perform_reasoning_step(self, agent_input: str, query:str) -> str:\n",
    "      \"\"\"Handle the reasoning operation during RAG.\"\"\"\n",
    "      reasoning_prompt = REASONING_PROMPT.format(\n",
    "          question=query,\n",
    "          agent_input=self.agent_input\n",
    "      )\n",
    "      self.agent_input += reasoning_prompt\n",
    "            # print(thought_prompt)\n",
    "\n",
    "      x ={\n",
    "              'prompt': reasoning_prompt,\n",
    "              'model' : 'llama-3.1-70b-versatile'\n",
    "        }\n",
    "\n",
    "      df = pd.DataFrame([x], index = [0])\n",
    "      print(df)\n",
    "      # Use LLM to decide if the next step should be retrieval or reasoning\n",
    "      t = pw.debug.table_from_pandas(df)\n",
    "      r = t.select(ret=self.llm(pw.xpacks.llm.llms.prompt_chat_single_qa(t.prompt), model=t.model))\n",
    "      reasoning_response = capture_output(r)[3:]\n",
    "      return reasoning_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "id": "ZC4v1_xTN3Mj"
   },
   "outputs": [],
   "source": [
    "# Example sample query\n",
    "sample_query = \"What is Pathway Endterm Guidelines ?\"\n",
    "\n",
    "# Create a DataFrame or table to mimic input format (pw.Table)\n",
    "import pandas as pd\n",
    "pw_ai_queries = pd.DataFrame({\n",
    "    'prompt': [sample_query],\n",
    "    'filters': [{}]  # You can add specific filters if needed, or leave empty\n",
    "})\n",
    "\n",
    "query = pw.debug.table_from_pandas(pw_ai_queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FSg_kDbRE2Ie"
   },
   "source": [
    "### Dynamic Tool Set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "id": "kO1m_JosBzPz"
   },
   "outputs": [],
   "source": [
    "#Dynamic Tool Set\n",
    "document_paths = []\n",
    "\n",
    "def rag_agent(query, agent) -> str:\n",
    "    \"\"\"\n",
    "    tool: rag_agent\n",
    "    description: Used for answering query on the basis of the context retrieved from the user provided documents.\n",
    "    Args:\n",
    "        query (str): The query to be passed to the RAG agent.\n",
    "            Example: \"What was Nike's gross margin in FY 2022?\"\n",
    "    Returns:\n",
    "        str: The response from the RAG agent.\n",
    "    \"\"\"\n",
    "\n",
    "    global document_paths\n",
    "    document_paths.append(agent.path)\n",
    "\n",
    "    if len(document_paths)>=2 and document_paths[-1] != document_paths[-2]:\n",
    "        gc.collect()\n",
    "        new_agent = RAGAGENT(llm=llm, embedding_dim=1024, thought_agent_prompt=thought_agent_prompt, reasoning_agent_prompt=reasoning_agent_prompt, max_steps=10, path = agent.path)\n",
    "        new_agent.engine = RetrieverQueryEngine.from_args(agent.retriever, llm=llm)\n",
    "        new_agent.retriever = agent.retriever\n",
    "        agent = new_agent\n",
    "        agent.cache_index = DynamicCacheIndex(dim=agent.embedding_dim, batch_size=16)\n",
    "\n",
    "    res = agent.run(query ,True)\n",
    "    score = chat_llm.invoke(confidence_prompt.format(steps = agent.agent_input, answer = res)).content\n",
    "    if agent.cache_index.process_pending_additions():\n",
    "      pass\n",
    "    return str(res) + \", Confidence Score : \" + str(score), agent\n",
    "\n",
    "#web search agent that uses Tavilly Web Search Tool.\n",
    "def web_search_agent(query):\n",
    "  \"\"\"\n",
    "  Used to get general online available information from the online source.\n",
    "  Args :- query (str) - Query to be search online eg :- \"Who is prime minister of Indi\"\n",
    "  \"\"\"\n",
    "  websearch = TavilyToolSpec(api_key= \"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\")\n",
    "  results = websearch.search(query, max_results = 3)\n",
    "  result = [r.text for r in results ]\n",
    "  promptt = ChatPromptTemplate.from_template(websearch_prompt)\n",
    "  chain = promptt | (guardrails | chat_llm) | output_parser\n",
    "  res=chain.invoke({\"query\": query, \"response\": result})\n",
    "  return res\n",
    "\n",
    "\n",
    "def end_tool(query):\n",
    "  \"\"\"\n",
    "  Used to end the question-answering process. Returns \"end\" when correctly executed.\n",
    "\n",
    "  Args : query(str) - this MUST be \"end\".\n",
    "\n",
    "  Returns :\n",
    "    str : \"end\"\n",
    "  \"\"\"\n",
    "  return \"end\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "id": "nYrFTK1upHKm"
   },
   "outputs": [],
   "source": [
    "# Tools are implemented as LlamaIndex FunctionTool entities.\n",
    "\n",
    "tool_map = {\n",
    "    'rag_agent': rag_agent,\n",
    "    \"web_search_agent\" : web_search_agent,\n",
    "    \"END\": end_tool\n",
    "}\n",
    "tools = [rag_agent,web_search_agent, end_tool]\n",
    "l_tools = []\n",
    "for tool in tools:\n",
    "    tool = FunctionTool.from_defaults(tool)\n",
    "    l_tools.append(tool)\n",
    "tools=[]\n",
    "tools_aux = []\n",
    "for i in l_tools:\n",
    "  tools.append(i.metadata.description)\n",
    "  tools_aux.append(i.metadata.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X7Bji46_E2If"
   },
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8Cwl2HLDE2If",
    "outputId": "fd19c46d-17b6-497d-9860-d01b144c769b"
   },
   "outputs": [],
   "source": [
    "# Provide the path to the pdf and the query.\n",
    "path_input = str(input(\"Enter the path of the pdf : \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "id": "Gw9sSr2WVud5"
   },
   "outputs": [],
   "source": [
    "supervisor = CodeandReasoningAgent(tools, tools_aux, chat_llm, tool_map, \"/content/VirtuosoSurgicalInc_20191227_1-A_EX1A-6 MAT CTRCT_11933379_EX1A-6 MAT CTRCT_License Agreement.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "id": "MwxRvtDDE2Ig"
   },
   "outputs": [],
   "source": [
    "# Example sample query\n",
    "sample_query = \"What is Pathway MidTerm Guidelines ?\"\n",
    "\n",
    "# Create a DataFrame or table to mimic input format (pw.Table)\n",
    "import pandas as pd\n",
    "pw_ai_queries = pd.DataFrame({\n",
    "    'prompt': [sample_query],\n",
    "    'filters': [{}]  # You can add specific filters if needed, or leave empty\n",
    "})\n",
    "\n",
    "query = pw.debug.table_from_pandas(pw_ai_queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zMgccJsKv7YF"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "LRDiEOT0Rlcy",
    "ndfrU9xsJBhj",
    "4RAxkMI-SA9c",
    "ieul7ZVXb0yT",
    "DDGsuiT9b30p",
    "lgJ1yyqtb60Z",
    "EIfo-jSzppq6",
    "oa36-OZY2Rsv"
   ],
   "machine_shape": "hm",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
